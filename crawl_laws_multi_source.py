#!/usr/bin/env python3
"""
å¤šæºæ³•å¾‹å’Œè¡Œæ”¿æ³•è§„çˆ¬è™«
ç»•è¿‡WAFæ‹¦æˆªï¼Œä»å¤šä¸ªæƒå¨æºè·å–æ³•å¾‹åŸæ–‡

æ•°æ®æºä¼˜å…ˆçº§ï¼š
1. å…¨å›½äººå¤§å®˜ç½‘ (npc.gov.cn) - æ³•å¾‹
2. å›½åŠ¡é™¢å®˜ç½‘ (gov.cn) - è¡Œæ”¿æ³•è§„
3. å›½å®¶ç¨åŠ¡æ€»å±€ - å¤‡ç”¨
"""

import asyncio
import re
import logging
from datetime import datetime
from pymongo import MongoClient
from playwright.async_api import async_playwright

# MongoDBé…ç½®
MONGO_URI = 'mongodb://localhost:27017/'
MONGO_DB = 'shared_cfo'
MONGO_COLLECTION = 'policies'

# ç›®æ ‡æ³•å¾‹åˆ—è¡¨ï¼ˆå…¨å›½äººå¤§å®˜ç½‘URLï¼‰
TARGET_LAWS = {
    'å¢å€¼ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20241225a5a9a09.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'category': 'å®ä½“ç¨',
        'tax_type': ['å¢å€¼ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½å¢å€¼ç¨æ³•',
    },
    'ä¸ªäººæ‰€å¾—ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20180831a48f9d9.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'category': 'å®ä½“ç¨',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨æ³•',
    },
    'ä¼ä¸šæ‰€å¾—ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20070316a0e510e.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'category': 'å®ä½“ç¨',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•',
    },
    'ç¨æ”¶å¾æ”¶ç®¡ç†æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20150427a4c7c2e.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'category': 'ç¨‹åºç¨',
        'tax_type': ['ç¨æ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•',
    },
}

# ç›®æ ‡è¡Œæ”¿æ³•è§„ï¼ˆå›½åŠ¡é™¢å®˜ç½‘URLï¼‰
TARGET_REGULATIONS = {
    'å¢å€¼ç¨æš‚è¡Œæ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2017-12/29/content_5343642.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'category': 'å®ä½“ç¨',
        'tax_type': ['å¢å€¼ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½å¢å€¼ç¨æš‚è¡Œæ¡ä¾‹',
    },
    'ä¸ªäººæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2018-12/22/content_5350262.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'category': 'å®ä½“ç¨',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹',
    },
    'ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2007-12/11/content_5279817.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'category': 'å®ä½“ç¨',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹',
    },
    'ç¨æ”¶å¾æ”¶ç®¡ç†æ³•å®æ–½ç»†åˆ™': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2016-02/06/content_5031145.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'category': 'ç¨‹åºç¨',
        'tax_type': ['ç¨æ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•å®æ–½ç»†åˆ™',
    },
}


class MultiSourceLawsCrawler:
    """å¤šæºæ³•å¾‹çˆ¬è™«"""

    def __init__(self):
        self.client = MongoClient(MONGO_URI)
        self.db = self.client[MONGO_DB]
        self.collection = self.db[MONGO_COLLECTION]
        self.results = []

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

    def extract_fields(self, title, content):
        """æå–æ”¿ç­–å­—æ®µ"""
        # æå–å‘æ–‡å­—å·
        document_number = None
        patterns = [
            r'ä¸»å¸­ä»¤[ç¬¬ç¬¬]?(\d+)å·',
            r'å›½åŠ¡é™¢ä»¤[ç¬¬ç¬¬]?(\d+)å·',
        ]
        for pattern in patterns:
            match = re.search(pattern, content[:500])
            if match:
                document_number = match.group(0)
                break

        # æå–å‘å¸ƒæ—¥æœŸ
        publish_date = None
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, content[:1000])
            if match:
                publish_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                break

        # æå–ç”Ÿæ•ˆæ—¥æœŸ
        effective_date = None
        eff_patterns = [
            r'è‡ª(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥èµ·æ–½è¡Œ',
            r'è‡ª(\d{4})-(\d{1,2})-(\d{1,2})èµ·æ–½è¡Œ',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥èµ·æ–½è¡Œ',
        ]
        for pattern in eff_patterns:
            match = re.search(pattern, content)
            if match:
                effective_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                break

        return {
            'document_number': document_number,
            'publish_date': publish_date,
            'effective_date': effective_date,
        }

    async def crawl_npc_law(self, name, info, browser):
        """ä»å…¨å›½äººå¤§å®˜ç½‘çˆ¬å–æ³•å¾‹"""
        try:
            url = info['npc_url']
            self.logger.info(f"[äººå¤§] çˆ¬å–: {name} -> {url}")

            page = await browser.new_page()
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)

            # è·å–é¡µé¢å†…å®¹
            content = ''
            selectors = [
                '.zwx3-box',      # äººå¤§å®˜ç½‘æ­£æ–‡
                '.content',
                '.article-content',
                'article',
            ]

            for selector in selectors:
                try:
                    elem = await page.query_selector(selector)
                    if elem:
                        content = await elem.inner_text()
                        if len(content) > 500:
                            break
                except:
                    continue

            if not content:
                content = await page.inner_text('body')

            await page.close()

            if len(content) < 200:
                self.logger.warning(f"[äººå¤§] {name}: å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                return None

            title = info['expected_title']
            fields = self.extract_fields(title, content)
            policy_id = f"NPC_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': title,
                'source': 'å…¨å›½äººå¤§',
                'url': url,
                'content': content,
                'document_level': info['level'],
                'document_type': info['type'],
                'tax_category': info['category'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'publish_date': fields['publish_date'],
                'document_number': fields['document_number'],
                'effective_date': fields['effective_date'],
                'crawled_at': datetime.now(),
                'quality_score': 5,
                'crawl_source': 'npc',
            }

            self.logger.info(f"[äººå¤§] {name}: æˆåŠŸ ({len(content)} å­—ç¬¦)")
            return policy_data

        except Exception as e:
            self.logger.error(f"[äººå¤§] {name}: å¤±è´¥ - {e}")
            return None

    async def crawl_gov_regulation(self, name, info, browser):
        """ä»å›½åŠ¡é™¢å®˜ç½‘çˆ¬å–è¡Œæ”¿æ³•è§„"""
        try:
            url = info['gov_url']
            self.logger.info(f"[å›½åŠ¡é™¢] çˆ¬å–: {name} -> {url}")

            page = await browser.new_page()
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)

            # è·å–é¡µé¢å†…å®¹
            content = ''
            selectors = [
                '.content-text',       # å›½åŠ¡é™¢å®˜ç½‘æ­£æ–‡
                '.article-content',
                '.content',
                '.texts',
                'article',
            ]

            for selector in selectors:
                try:
                    elem = await page.query_selector(selector)
                    if elem:
                        content = await elem.inner_text()
                        if len(content) > 500:
                            break
                except:
                    continue

            if not content:
                content = await page.inner_text('body')

            await page.close()

            if len(content) < 200:
                self.logger.warning(f"[å›½åŠ¡é™¢] {name}: å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                return None

            title = info['expected_title']
            fields = self.extract_fields(title, content)
            policy_id = f"GOV_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': title,
                'source': 'å›½åŠ¡é™¢',
                'url': url,
                'content': content,
                'document_level': info['level'],
                'document_type': info['type'],
                'tax_category': info['category'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'publish_date': fields['publish_date'],
                'document_number': fields['document_number'],
                'effective_date': fields['effective_date'],
                'crawled_at': datetime.now(),
                'quality_score': 5,
                'crawl_source': 'gov',
            }

            self.logger.info(f"[å›½åŠ¡é™¢] {name}: æˆåŠŸ ({len(content)} å­—ç¬¦)")
            return policy_data

        except Exception as e:
            self.logger.error(f"[å›½åŠ¡é™¢] {name}: å¤±è´¥ - {e}")
            return None

    async def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰ç›®æ ‡æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)

            self.logger.info("=" * 60)
            self.logger.info("å¼€å§‹çˆ¬å–æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„ï¼ˆå¤šæºï¼‰")
            self.logger.info("=" * 60)

            # çˆ¬å–æ³•å¾‹ï¼ˆå…¨å›½äººå¤§ï¼‰
            self.logger.info("\n>>> ç¬¬ä¸€éƒ¨åˆ†ï¼šçˆ¬å–æ³•å¾‹ï¼ˆå…¨å›½äººå¤§å®˜ç½‘ï¼‰")
            for name, info in TARGET_LAWS.items():
                policy = await self.crawl_npc_law(name, info, browser)
                if policy:
                    self.save_policy(policy)
                    self.results.append({
                        'name': name,
                        'title': policy['title'],
                        'source': 'å…¨å›½äººå¤§',
                        'level': policy['document_level'],
                        'content_length': len(policy['content']),
                    })
                await asyncio.sleep(2)  # å»¶è¿Ÿ

            # çˆ¬å–è¡Œæ”¿æ³•è§„ï¼ˆå›½åŠ¡é™¢ï¼‰
            self.logger.info("\n>>> ç¬¬äºŒéƒ¨åˆ†ï¼šçˆ¬å–è¡Œæ”¿æ³•è§„ï¼ˆå›½åŠ¡é™¢å®˜ç½‘ï¼‰")
            for name, info in TARGET_REGULATIONS.items():
                policy = await self.crawl_gov_regulation(name, info, browser)
                if policy:
                    self.save_policy(policy)
                    self.results.append({
                        'name': name,
                        'title': policy['title'],
                        'source': 'å›½åŠ¡é™¢',
                        'level': policy['document_level'],
                        'content_length': len(policy['content']),
                    })
                await asyncio.sleep(2)  # å»¶è¿Ÿ

            await browser.close()

        self.logger.info("\n" + "=" * 60)
        self.logger.info("çˆ¬å–å®Œæˆ!")
        self.logger.info("=" * 60)

    def save_policy(self, policy_data):
        """ä¿å­˜æ”¿ç­–åˆ°æ•°æ®åº“"""
        # å…ˆåˆ é™¤åŒåçš„æ—§æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
        self.collection.delete_many({'title': policy_data['title']})
        # æ’å…¥æ–°æ•°æ®
        self.collection.insert_one(policy_data)

    def print_results(self):
        """æ‰“å°ç»“æœç»Ÿè®¡"""
        print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"æˆåŠŸ: {len(self.results)} æ¡")

        for r in self.results:
            print(f"  â€¢ [{r['level']}] {r['name']}")
            print(f"    æ¥æº: {r['source']}")
            print(f"    å†…å®¹é•¿åº¦: {r['content_length']} å­—ç¬¦")


async def main():
    """ä¸»å‡½æ•°"""
    crawler = MultiSourceLawsCrawler()
    await crawler.crawl_all()
    crawler.print_results()


if __name__ == '__main__':
    asyncio.run(main())
