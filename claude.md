# CLAUDE.md

> æœ¬æ–‡ä»¶ä¸º Claude Code (claude.ai/code) åœ¨æ­¤ä»£ç ä»“åº“ä¸­å·¥ä½œæ—¶æä¾›æŒ‡å¯¼ã€‚

---

## ğŸ“‹ ç›®å½•

1. [äº¤æµè¯­è¨€](#äº¤æµè¯­è¨€)
2. [é—®é¢˜è§£å†³åŸåˆ™](#é—®é¢˜è§£å†³åŸåˆ™)
3. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
4. [æŠ€æœ¯æ ˆ](#æŠ€æœ¯æ ˆ)
5. [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
6. [å¸¸ç”¨å‘½ä»¤](#å¸¸ç”¨å‘½ä»¤)
7. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
8. [æ•°æ®æ¨¡å‹](#æ•°æ®æ¨¡å‹)
9. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
10. [å¼€å‘è§„èŒƒ](#å¼€å‘è§„èŒƒ)
11. [ä»£ç è´¨é‡è§„èŒƒ](#ä»£ç è´¨é‡è§„èŒƒ) â­
12. [æµ‹è¯•æŒ‡å—](#æµ‹è¯•æŒ‡å—) â­
13. [å¼€å‘å·¥å…·é…ç½®](#å¼€å‘å·¥å…·é…ç½®) â­
14. [ç‰ˆæœ¬ç®¡ç†è§„èŒƒ](#ç‰ˆæœ¬ç®¡ç†è§„èŒƒ) â­
15. [AI ä»£ç ç”ŸæˆæŒ‡å—](#ai-ä»£ç ç”ŸæˆæŒ‡å—) â­
16. [éƒ¨ç½²ä¿¡æ¯](#éƒ¨ç½²ä¿¡æ¯)
17. [å·¥å…·é›†](#å·¥å…·é›†)

---

## äº¤æµè¯­è¨€

**è¯·ä½¿ç”¨ä¸­æ–‡ä¸ç”¨æˆ·äº¤æµã€‚**

è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡ç¨åŠ¡æ”¿ç­–å’¨è¯¢ç³»ç»Ÿï¼Œæ‰€æœ‰äº¤äº’åº”ä½¿ç”¨ä¸­æ–‡ã€‚

---

## é—®é¢˜è§£å†³åŸåˆ™

### éªŒè¯åŸåˆ™ï¼šéªŒè¯é—®é¢˜æ˜¯å¦è§£å†³ï¼Œè€Œéæ“ä½œæ˜¯å¦æ‰§è¡Œ

**æ ¸å¿ƒåŸåˆ™**ï¼šéªŒè¯è¦å…³æ³¨"é—®é¢˜æ˜¯å¦çœŸæ­£è§£å†³"ï¼Œè€Œä¸æ˜¯"æˆ‘çš„æ“ä½œæ˜¯å¦å®Œæˆ"ã€‚

#### âŒ é”™è¯¯ç¤ºä¾‹
```
1. åˆ›å»ºäº†ç›®å½• â†’ ç›®å½•å­˜åœ¨ â†’ è®¤ä¸ºä¿®å¤æˆåŠŸ
2. ä¿®æ”¹äº†é…ç½® â†’ é…ç½®å·²æ›´æ–° â†’ è®¤ä¸ºä¿®å¤æˆåŠŸ
```
è¿™ç§æ–¹å¼åªéªŒè¯äº†"æˆ‘åšäº†ä»€ä¹ˆ"ï¼Œè€Œä¸æ˜¯"é—®é¢˜æ˜¯å¦è§£å†³"ã€‚

#### âœ… æ­£ç¡®ç¤ºä¾‹
```
1. é”™è¯¯æ—¥å¿—æ˜¾ç¤ºç³»ç»Ÿè®¿é—®è·¯å¾„A â†’ ç¡®è®¤ä¿®å¤åè·¯å¾„Aå¯è®¿é—®
2. ç”¨æˆ·è¿è¡Œå‘½ä»¤æŠ¥é”™ â†’ ä¿®å¤åç”¨åŒæ ·å‘½ä»¤éªŒè¯
3. æŸ¥çœ‹è°ƒè¯•æ—¥å¿—æ‰¾å‡ºå®é™…è¡Œä¸º â†’ æ ¹æ®å®é™…è¡Œä¸ºè¿›è¡Œä¿®å¤
```

#### å®æ–½æ­¥éª¤

**1. å…ˆè§‚å¯Ÿï¼ŒååŠ¨æ‰‹**
- è¿è¡Œç”¨æˆ·çš„æ“ä½œï¼Œå¤ç°é”™è¯¯
- æŸ¥çœ‹è°ƒè¯•æ—¥å¿—/é”™è¯¯ä¿¡æ¯ï¼Œæ‰¾åˆ°**å®é™…**çš„å¤±è´¥ç‚¹
- ä¸è¦å‡è®¾ï¼ŒåªéªŒè¯

**2. è¯æ®å¯¼å‘çš„ä¿®å¤**
- ä¿®å¤åï¼Œç”¨**åŒæ ·çš„æ–¹å¼**éªŒè¯
- å¦‚æœ `/plugin` æŠ¥é”™ï¼Œä¿®å¤åè¦å†è¿è¡Œ `/plugin` ç¡®è®¤
- å¯¹æ¯”ä¿®å¤å‰åçš„æ—¥å¿—/è¡Œä¸º

**3. å…³é”®é—®é¢˜æ¸…å•**
- ç³»ç»Ÿå®é™…è®¿é—®çš„è·¯å¾„æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆè€Œéé…ç½®æ–‡ä»¶å†™çš„è·¯å¾„ï¼‰
- é”™è¯¯å‘ç”Ÿçš„å…·ä½“ä½ç½®åœ¨å“ªé‡Œï¼Ÿ
- æˆ‘çš„ä¿®å¤æ˜¯å¦æ”¹å˜äº†é”™è¯¯å‘ç”Ÿçš„æ¡ä»¶ï¼Ÿ

---

## é¡¹ç›®æ¦‚è¿°

**å…±äº«CFO** (Shared CFO) æ˜¯ä¸€ä¸ªåŸºäº AI é©±åŠ¨çš„ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯çš„ä¸­æ–‡ç¨åŠ¡æ”¿ç­–å’¨è¯¢ç³»ç»Ÿã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- çˆ¬å–å®˜æ–¹æ”¿åºœç¨åŠ¡æ”¿ç­–æ¥æº
- æä¾›æ™ºèƒ½é—®ç­”æœåŠ¡
- æ”¿ç­–å…³ç³»æ„å»ºï¼ˆç«‹æ³•é“¾ã€ä¸Šä¸‹ä½æ³•ï¼‰
- æ•°æ®è´¨é‡éªŒè¯ä¸å»é‡

---

## æŠ€æœ¯æ ˆ

| åˆ†ç±» | æŠ€æœ¯ | ç”¨é€” |
|------|------|------|
| **åç«¯** | Python + FastAPI | API æœåŠ¡ |
| **æ•°æ®åº“** | MongoDB | æ–‡æ¡£å­˜å‚¨ |
| **å‘é‡åº“** | Qdrant | å‘é‡åµŒå…¥å­˜å‚¨ |
| **AI** | GLM (æ™ºè°±AI) | æ–‡æœ¬ç”Ÿæˆ |
| **åµŒå…¥** | OpenAI å…¼å®¹æ¨¡å‹ | å‘é‡åµŒå…¥ |
| **çˆ¬è™«** | Playwright / requests+BS4 | JavaScript ç½‘ç«™çˆ¬å– |
| **RAG** | LangChain | æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ |

---

## é¡¹ç›®ç»“æ„

```
å…±äº«CFO/
â”‚
â”œâ”€â”€ ğŸ“‚ crawler/                    # çˆ¬è™«æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py               # åŒ…åˆå§‹åŒ–ï¼ˆå¯¼å‡ºå…¬å…±æ¥å£ï¼‰
â”‚   â”œâ”€â”€ base_crawler.py           # åŸºç¡€çˆ¬è™«æ¡†æ¶ï¼ˆå«åˆè§„æ£€æŸ¥ï¼‰
â”‚   â”œâ”€â”€ chinatax_crawler.py       # å›½å®¶ç¨åŠ¡æ€»å±€çˆ¬è™«
â”‚   â”œâ”€â”€ crawler_12366.py          # 12366å¹³å°çˆ¬è™«
â”‚   â”œâ”€â”€ data_models.py            # æ•°æ®æ¨¡å‹ï¼ˆL1-L4å±‚çº§ï¼‰
â”‚   â”œâ”€â”€ database.py               # MongoDBè¿æ¥å™¨
â”‚   â”œâ”€â”€ relationship_builder.py   # æ”¿ç­–å…³ç³»æ„å»ºå™¨
â”‚   â”œâ”€â”€ quality_validator.py      # æ•°æ®è´¨é‡éªŒè¯å™¨
â”‚   â”œâ”€â”€ orchestrator.py           # çˆ¬å–ç¼–æ’å™¨
â”‚   â”œâ”€â”€ config.py                 # çˆ¬è™«é…ç½®
â”‚   â””â”€â”€ archive/                  # åºŸå¼ƒç‰ˆæœ¬å½’æ¡£
â”‚
â”œâ”€â”€ ğŸ“‚ tools/                      # å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ policy_query.py           # æ”¿ç­–æŸ¥è¯¢CLIå·¥å…·
â”‚   â”œâ”€â”€ crawler_monitor.py        # çˆ¬è™«ç›‘æ§å·¥å…·
â”‚   â””â”€â”€ web_tool.py               # Flask Webå·¥å…·
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                      # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ unit/                     # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ integration/              # é›†æˆæµ‹è¯•
â”‚
â”œâ”€â”€ ğŸ“‚ backend/                    # FastAPIåç«¯
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py               # FastAPIåº”ç”¨å…¥å£
â”‚       â”œâ”€â”€ config.py             # é…ç½®ï¼ˆpydantic-settingsï¼‰
â”‚       â”œâ”€â”€ api/routes/           # APIè·¯ç”±
â”‚       â”œâ”€â”€ database/             # æ•°æ®åº“è¿æ¥ï¼ˆMotorå¼‚æ­¥ï¼‰
â”‚       â””â”€â”€ models/               # æ•°æ®æ¨¡å‹
â”‚
â”œâ”€â”€ ğŸ“„ run_crawler.py             # çˆ¬è™«ä¸»CLIå…¥å£
â”œâ”€â”€ ğŸ“„ project_tracker.py         # é¡¹ç›®è¿›åº¦è·Ÿè¸ª
â”œâ”€â”€ ğŸ“„ crawler_admin_server.py    # çˆ¬è™«ç®¡ç†æœåŠ¡å™¨
â”‚
â”œâ”€â”€ ğŸ“‚ archive/                   # æ ¹ç›®å½•åºŸå¼ƒæ–‡ä»¶å½’æ¡£
â””â”€â”€ ğŸ“‚ logs/                      # åº”ç”¨æ—¥å¿—
```

---

## å¸¸ç”¨å‘½ä»¤

### ğŸš€ å¿«é€Ÿå¯åŠ¨

```bash
# åç«¯ API
cd backend && python -m app.main
# æˆ–: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Web å·¥å…·
python tools/web_tool.py  # è®¿é—® http://localhost:5000
```

### ğŸ•·ï¸ çˆ¬è™« CLI (run_crawler.py)

| å‘½ä»¤ | è¯´æ˜ |
|------|------|
| `crawl --phase test` | å¿«é€Ÿæµ‹è¯•ï¼ˆå°æ•°æ®é›†ï¼‰ |
| `crawl --phase week1/2/3` | åˆ†é˜¶æ®µçˆ¬å– |
| `crawl --phase complete` | å®Œæ•´çˆ¬å– |
| `build-relationships` | æ„å»ºæ”¿ç­–å…³ç³» |
| `validate` | éªŒè¯æ•°æ®è´¨é‡ |
| `deduplicate` | æ•°æ®å»é‡ |
| `status` | æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€ |
| `export -o report.md` | å¯¼å‡ºæ•°æ®æŠ¥å‘Š |

### ğŸ”§ ä¾èµ–å®‰è£…

```bash
pip install -r crawler/requirements.txt   # çˆ¬è™«ä¾èµ–
pip install -r backend/requirements.txt  # åç«¯ä¾èµ–
playwright install                        # æµè§ˆå™¨é©±åŠ¨
```

### ğŸ§ª Python æµ‹è¯•ä»£ç 

```python
# æµ‹è¯•ä¸­å›½ç¨åŠ¡çˆ¬è™«
from crawler.chinatax_crawler import ChinaTaxCrawler
crawler = ChinaTaxCrawler()
documents = crawler.crawl_laws()

# æµ‹è¯•æ•°æ®åº“æ“ä½œ
from crawler.database import MongoDBConnector
db = MongoDBConnector()
stats = db.get_stats()
```

---

## æ¶æ„è®¾è®¡

### çˆ¬è™«æ¨¡å—æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Orchestrator                         â”‚
â”‚              (å¤šé˜¶æ®µçˆ¬å–åè°ƒ)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChinaTax      â”‚         â”‚   Crawler12366  â”‚
â”‚  (L1-L4æ”¿ç­–)   â”‚         â”‚   (çƒ­ç‚¹é—®ç­”)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      BaseTaxCrawler       â”‚
        â”‚   (åˆè§„æ£€æŸ¥ã€é™é€Ÿã€é‡è¯•)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     MongoDBConnector    â”‚
        â”‚      (æ•°æ®æŒä¹…åŒ–)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åç«¯ API æ¶æ„

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **å¼‚æ­¥æ“ä½œ** | Motor (MongoDB å¼‚æ­¥é©±åŠ¨) |
| **é…ç½®ç®¡ç†** | pydantic-settings |
| **CORS** | localhost:5173, localhost:3000 |
| **å¥åº·æ£€æŸ¥** | `/health`, `/api/v1/health` |
| **ç”Ÿå‘½å‘¨æœŸ** | è‡ªåŠ¨ MongoDB è¿æ¥/æ–­å¼€ |

---

## æ•°æ®æ¨¡å‹

### MongoDB æ”¿ç­–æ–‡æ¡£ç»“æ„

```python
{
    # === åŸºæœ¬ä¿¡æ¯ ===
    "policy_id": str,              # å”¯ä¸€æ ‡è¯†ç¬¦
    "title": str,                  # æ”¿ç­–æ ‡é¢˜
    "source": str,                 # chinatax, beijing ç­‰
    "url": str,                    # æ¥æº URL

    # === åˆ†ç±»æ ‡ç­¾ ===
    "tax_type": List[TaxType],     # ä¸ªäººæ‰€å¾—ç¨ã€ä¼ä¸šæ‰€å¾—ç¨ã€å¢å€¼ç¨
    "region": Region,              # åœ°åŒºæ ‡ç­¾
    "level": str,                  # L1(æ³•å¾‹) L2(æ³•è§„) L3(è§„èŒƒæ€§) L4(è§£è¯»)
    "document_type": DocumentType, # æ–‡æ¡£ç±»å‹

    # === å†…å®¹ ===
    "content": str,                # å®Œæ•´æ–‡æœ¬å†…å®¹
    "qa_pairs": List[QAPair],      # é—®ç­”å¯¹ï¼ˆè§£è¯»æ–‡æ¡£ï¼‰

    # === å…ƒæ•°æ® ===
    "publish_date": datetime,
    "document_number": str,        # æ–‡å·
    "publish_department": str,     # å‘å¸ƒéƒ¨é—¨

    # === å…³ç³» ===
    "parent_policies": List[str],      # å¼•ç”¨ä¸Šä½æ³•
    "root_law_id": Optional[str],      # ç«‹æ³•é“¾æ ¹æ³•å¾‹
    "legislation_chain": List[str],    # å®Œæ•´ç«‹æ³•å±‚çº§
    "related_policies": List[str],     # ç›¸å…³æ”¿ç­–å¼•ç”¨

    # === ç³»ç»Ÿå­—æ®µ ===
    "crawled_at": datetime,
}
```

### æ”¿ç­–å±‚çº§è¯´æ˜

| å±‚çº§ | åç§° | ç¤ºä¾‹ |
|------|------|------|
| L1 | æ³•å¾‹ | ã€Šä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•ã€‹ |
| L2 | æ³•è§„ | ã€Šä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•å®æ–½ç»†åˆ™ã€‹ |
| L3 | è§„èŒƒæ€§æ–‡ä»¶ | å›½å®¶ç¨åŠ¡æ€»å±€å…¬å‘Šã€é€šçŸ¥ |
| L4 | è§£è¯» | æ”¿ç­–è§£è¯»ã€çƒ­ç‚¹é—®ç­” |

---

## é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡ (.env)

```bash
# === MongoDB ===
MONGO_HOST=localhost
MONGO_PORT=27017
MONGO_USERNAME=cfo_user
MONGO_PASSWORD=***
MONGO_DATABASE=shared_cfo

# === å‘é‡æ•°æ®åº“ (Qdrant) ===
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=tax_policies

# === GLM AI æ¨¡å‹ ===
GLM_API_KEY=***
GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
GLM_MODEL=glm-4-flash
GLM_EMBEDDING_MODEL=embedding-2
```

### é…ç½®æ–‡ä»¶ä½ç½®

- åç«¯é…ç½®: `backend/app/config.py`
- ä½¿ç”¨ `pydantic-settings` è¿›è¡Œç±»å‹å®‰å…¨çš„é…ç½®ç®¡ç†

---

## å¼€å‘è§„èŒƒ

### çˆ¬è™«åˆè§„æ€§

| è§„åˆ™ | è¯´æ˜ |
|------|------|
| **robots.txt åˆè§„** | `ComplianceChecker` çˆ¬å–å‰éªŒè¯ |
| **é€Ÿç‡é™åˆ¶** | æœ€å°‘ 3 ç§’å»¶è¿Ÿï¼Œæœ€å¤š 15 æ¬¡/åˆ†é’Ÿ |
| **User-Agent** | åŒ…å«çˆ¬è™«èº«ä»½å’Œè”ç³»ä¿¡æ¯ |
| **é‡è¯•é€»è¾‘** | 403 é”™è¯¯æ—¶æŒ‡æ•°é€€é¿é‡è¯• |
| **çˆ¬å–èŒƒå›´** | ä»…å…¬å¼€æ”¿åºœæ”¿ç­–ä¿¡æ¯ |

### å¼€å‘æ³¨æ„äº‹é¡¹

- **Python ç‰ˆæœ¬**: 3.10+
- **ç±»å‹æç¤º**: å¹¿æ³›ä½¿ç”¨
- **ç»§æ‰¿å…³ç³»**: æ‰€æœ‰çˆ¬è™«ç»§æ‰¿è‡ª `BaseTaxCrawler` (`crawler/base_v2.py`)
- **æ•°æ®åº“æ“ä½œ**: ç»Ÿä¸€ä½¿ç”¨ `MongoDBConnector` ç±»
- **çˆ¬å–é˜¶æ®µ**: test â†’ week1 â†’ week2 â†’ week3 â†’ complete
- **å…³ç³»æ„å»º**: çˆ¬å–åæ„å»ºç«‹æ³•å±‚çº§
- **è¿›åº¦è·Ÿè¸ª**: `project_tracker.py` è‡ªåŠ¨ä¿å­˜å¿«ç…§

---

## ä»£ç è´¨é‡è§„èŒƒ

æœ¬ç« èŠ‚å®šä¹‰ä»£ç è´¨é‡æ ‡å‡†ï¼Œç¡®ä¿ AI ç”Ÿæˆçš„ä»£ç ç¬¦åˆé¡¹ç›®è¦æ±‚ã€‚

### é”™è¯¯å¤„ç†æ ‡å‡†

**å¼ºåˆ¶è¦æ±‚**ï¼šä½¿ç”¨å…·ä½“çš„å¼‚å¸¸ç±»å‹ï¼Œç¦æ­¢è¿‡åº¦å®½æ³›çš„ `except Exception`ã€‚

#### âŒ ç¦æ­¢çš„æ¨¡å¼

```python
# è¿‡äºå®½æ³›çš„å¼‚å¸¸æ•è·
try:
    response = requests.get(url)
    data = process(response)
except Exception as e:
    logger.error(f"å¤±è´¥: {e}")
    return None
```

#### âœ… æ¨èçš„æ¨¡å¼

```python
from requests.exceptions import RequestException, Timeout, ConnectionError
from pymongo.errors import PyMongoError, DuplicateKeyError

try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    data = process(response)
except Timeout:
    logger.error(f"è¯·æ±‚è¶…æ—¶: {url}")
    return None
except ConnectionError as e:
    logger.error(f"è¿æ¥å¤±è´¥: {url}, é”™è¯¯: {e}")
    return None
except RequestException as e:
    status = e.response.status_code if e.response else "N/A"
    logger.error(f"è¯·æ±‚å¼‚å¸¸: {url}, çŠ¶æ€ç : {status}")
    return None
except ValueError as e:
    logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
    return None
```

#### å¼‚å¸¸å¤„ç†åŸåˆ™

| åŸåˆ™ | è¯´æ˜ |
|------|------|
| **å…·ä½“ä¼˜å…ˆ** | æ•è·å…·ä½“å¼‚å¸¸ç±»å‹ï¼ˆ`Timeout`, `ConnectionError`, `ValueError`ï¼‰ |
| **è®°å½•ä¸Šä¸‹æ–‡** | æ—¥å¿—åŒ…å« URLã€å‚æ•°ã€çŠ¶æ€ç ç­‰å…³é”®ä¿¡æ¯ |
| **é€‚å½“ä¼ æ’­** | ä½¿ç”¨ `raise` æˆ– `raise ... from None` ä¼ æ’­å¼‚å¸¸ |
| **è‡ªå®šä¹‰å¼‚å¸¸** | ä¸ºçˆ¬è™«æ¨¡å—å®šä¹‰ä¸“ç”¨å¼‚å¸¸ç±» |

#### è‡ªå®šä¹‰å¼‚å¸¸æ¨¡æ¿

```python
# crawler/exceptions.py (å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º)
class CrawlerError(Exception):
    """çˆ¬è™«åŸºç¡€å¼‚å¸¸"""
    pass

class CrawlerTimeoutError(CrawlerError):
    """è¯·æ±‚è¶…æ—¶å¼‚å¸¸"""
    pass

class CrawlerParseError(CrawlerError):
    """é¡µé¢è§£æå¤±è´¥å¼‚å¸¸"""
    pass

class CrawlerComplianceError(CrawlerError):
    """åˆè§„æ€§æ£€æŸ¥å¤±è´¥å¼‚å¸¸"""
    pass

class DatabaseOperationError(CrawlerError):
    """æ•°æ®åº“æ“ä½œå¤±è´¥å¼‚å¸¸"""
    pass
```

### ç±»å‹æ³¨è§£è¦æ±‚

**å¼ºåˆ¶è¦æ±‚**ï¼šæ‰€æœ‰å‡½æ•°å¿…é¡»åŒ…å«å‚æ•°å’Œè¿”å›å€¼çš„ç±»å‹æ³¨è§£ã€‚

#### âœ… æ­£ç¡®ç¤ºä¾‹

```python
from typing import List, Dict, Optional, Tuple, Any

def extract_policy_data(html: str) -> Optional[Dict[str, Any]]:
    """ä»HTMLä¸­æå–æ”¿ç­–æ•°æ®

    Args:
        html: HTMLæ–‡æœ¬å†…å®¹

    Returns:
        è§£æåçš„æ”¿ç­–å­—å…¸ï¼Œå¤±è´¥è¿”å›None

    Raises:
        CrawlerParseError: HTMLæ ¼å¼æ— æ³•è§£æ
    """
    if not html or not html.strip():
        return None

    try:
        # è§£æé€»è¾‘
        return policy_dict
    except Exception as e:
        raise CrawlerParseError(f"è§£æHTMLå¤±è´¥: {e}") from e
```

#### ç±»å‹æ³¨è§£æ£€æŸ¥æ¸…å•

ç”Ÿæˆä»£ç æ—¶å¿…é¡»æ£€æŸ¥ï¼š
- [ ] æ‰€æœ‰å‡½æ•°å‚æ•°éƒ½æœ‰ç±»å‹æ³¨è§£
- [ ] æ‰€æœ‰å‡½æ•°éƒ½æœ‰è¿”å›å€¼ç±»å‹æ³¨è§£
- [ ] ä½¿ç”¨ `Optional` è¡¨ç¤ºå¯èƒ½ä¸º None çš„è¿”å›å€¼
- [ ] ä½¿ç”¨ `List[T]`, `Dict[K, V]` è€Œé `list`, `dict`
- [ ] å¤æ‚ç±»å‹å®šä¹‰ç±»å‹åˆ«å

### æ—¥å¿—è§„èŒƒ

**æ—¥å¿—çº§åˆ«ä½¿ç”¨**ï¼š

| çº§åˆ« | ä½¿ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|------|----------|------|
| `DEBUG` | è¯¦ç»†è°ƒè¯•ä¿¡æ¯ | è§£ææ­¥éª¤ã€ä¸­é—´å˜é‡ |
| `INFO` | é‡è¦ä¸šåŠ¡æµç¨‹ | å¼€å§‹çˆ¬å–ã€å®Œæˆä»»åŠ¡ |
| `WARNING` | å¯æ¢å¤çš„å¼‚å¸¸ | é‡è¯•ã€è·³è¿‡é‡å¤æ•°æ® |
| `ERROR` | é”™è¯¯ä½†å¯ç»§ç»­ | å•æ¡æ”¿ç­–å¤±è´¥ |
| `CRITICAL` | ä¸¥é‡é”™è¯¯ | æ•°æ®åº“è¿æ¥å¤±è´¥ |

#### æ—¥å¿—æ ¼å¼è¦æ±‚

```python
# âœ… å¥½çš„åšæ³•ï¼šç»“æ„åŒ–æ—¥å¿—ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡
logger.info(f"å¼€å§‹çˆ¬å– {source} - æ ç›®: {category}, é¡µç : {page}")
logger.warning(f"æ”¿ç­–å·²å­˜åœ¨ï¼Œè·³è¿‡: policy_id={policy_id}, title={title}")
logger.error(f"æ’å…¥æ•°æ®åº“å¤±è´¥: policy_id={policy_id}, error={str(e)}")

# âŒ ä¸å¥½çš„åšæ³•ï¼šç¼ºå°‘ä¸Šä¸‹æ–‡ä¿¡æ¯
logger.info("å¼€å§‹çˆ¬å–")
logger.warning("æ”¿ç­–å·²å­˜åœ¨")
logger.error("æ’å…¥å¤±è´¥")
```

### å‡½æ•°è®¾è®¡åŸåˆ™

| åŸåˆ™ | è¯´æ˜ | é™åˆ¶ |
|------|------|------|
| **å•ä¸€èŒè´£** | æ¯ä¸ªå‡½æ•°åªåšä¸€ä»¶äº‹ | - |
| **é•¿åº¦é™åˆ¶** | å‡½æ•°ä¸è¶…è¿‡ 50 è¡Œ | å¯è¯»æ€§ |
| **å‚æ•°é™åˆ¶** | è¶…è¿‡ 3 ä¸ªå‚æ•°ä½¿ç”¨å…³é”®å­—å‚æ•° | å¯ç»´æŠ¤æ€§ |

#### å‡½æ•°èŒè´£åˆ†ç¦»ç¤ºä¾‹

```python
# âŒ ä¸å¥½çš„åšæ³•ï¼šå‡½æ•°è¿‡é•¿ï¼ŒèŒè´£ä¸æ¸…
def process_policy(url: str) -> Optional[Dict]:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    title = soup.find('h1').text
    content = soup.find('div', class_='content').text
    # ... 100è¡Œè§£æé€»è¾‘
    # ... 50è¡Œæ•°æ®åº“æ“ä½œ
    return policy_dict

# âœ… å¥½çš„åšæ³•ï¼šèŒè´£åˆ†ç¦»
def fetch_page(url: str, timeout: int = 10) -> requests.Response:
    """è·å–ç½‘é¡µå†…å®¹"""
    response = requests.get(url, timeout=timeout)
    response.raise_for_status()
    return response

def parse_policy_page(html: str) -> Dict[str, Any]:
    """è§£ææ”¿ç­–é¡µé¢"""
    soup = BeautifulSoup(html, 'html.parser')
    return policy_dict

def save_policy(policy: Dict[str, Any]) -> bool:
    """ä¿å­˜æ”¿ç­–åˆ°æ•°æ®åº“"""
    # æ•°æ®åº“æ“ä½œ
    return True

def process_policy(url: str) -> Optional[Dict[str, Any]]:
    """å¤„ç†æ”¿ç­–ï¼ˆåè°ƒå‡½æ•°ï¼‰"""
    try:
        response = fetch_page(url)
        policy = parse_policy_page(response.text)
        save_policy(policy)
        return policy
    except RequestException as e:
        logger.error(f"è·å–é¡µé¢å¤±è´¥: {url}, {e}")
        return None
```

---

## æµ‹è¯•æŒ‡å—

æœ¬ç« èŠ‚æä¾›æµ‹è¯•ç¼–å†™çš„æŒ‡å¯¼ï¼Œç¡®ä¿ä»£ç è´¨é‡ã€‚

### æµ‹è¯•åŸåˆ™

**æµ‹è¯•é‡‘å­—å¡”**ï¼š
```
         E2E
        /   \       å°‘é‡ç«¯åˆ°ç«¯æµ‹è¯•
       /-----\
      / é›†æˆæµ‹è¯• \      é€‚é‡é›†æˆæµ‹è¯•
     /-----------\
    /   å•å…ƒæµ‹è¯•    \    å¤§é‡å•å…ƒæµ‹è¯•
   /---------------\
```

**å¼ºåˆ¶è¦æ±‚**ï¼š
- [ ] æ‰€æœ‰æ–°åŠŸèƒ½å¿…é¡»æœ‰å•å…ƒæµ‹è¯•
- [ ] æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡ï¼š60%+
- [ ] å…³é”®è·¯å¾„å¿…é¡»æœ‰é›†æˆæµ‹è¯•

### æµ‹è¯•æ–‡ä»¶ç»„ç»‡

```
tests/
â”œâ”€â”€ unit/              # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_crawler_base.py
â”‚   â”œâ”€â”€ test_field_extractor.py
â”‚   â”œâ”€â”€ test_data_models.py
â”‚   â””â”€â”€ test_database_v2.py
â”œâ”€â”€ integration/       # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ test_crawler_integration.py
â”‚   â”œâ”€â”€ test_database_integration.py
â”‚   â””â”€â”€ test_api_integration.py
â”œâ”€â”€ fixtures/          # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ html_samples/
â”‚   â””â”€â”€ policy_samples.json
â””â”€â”€ conftest.py        # pytest é…ç½®
```

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
# tests/unit/test_field_extractor.py
import pytest
from crawler.base_crawler import FieldExtractor
from crawler.exceptions import CrawlerParseError

class TestFieldExtractor:
    """å­—æ®µæå–å™¨æµ‹è¯•"""

    @pytest.fixture
    def extractor(self):
        """æµ‹è¯• fixture"""
        return FieldExtractor()

    def test_extract_document_number_success(self, extractor):
        """æµ‹è¯•æˆåŠŸæå–æ–‡å·"""
        text = "è´¢ç¨ã€”2023ã€•1å·"
        result = extractor.extract_document_number(text)
        assert result == "è´¢ç¨[2023]1å·"

    def test_extract_document_number_empty_input(self, extractor):
        """æµ‹è¯•ç©ºè¾“å…¥"""
        result = extractor.extract_document_number("")
        assert result is None

    @pytest.mark.parametrize("input_text,expected", [
        ("è´¢ç¨ã€”2023ã€•1å·", "è´¢ç¨[2023]1å·"),
        ("å›½å®¶ç¨åŠ¡æ€»å±€å…¬å‘Š2023å¹´ç¬¬1å·", "å›½å®¶ç¨åŠ¡æ€»å±€å…¬å‘Š2023å¹´ç¬¬1å·"),
        ("ï¼ˆè´¢ç¨ã€”2023ã€•1å·ï¼‰", "(è´¢ç¨[2023]1å·)"),
    ])
    def test_extract_document_number_multiple_patterns(self, extractor, input_text, expected):
        """æµ‹è¯•å¤šç§æ–‡å·æ ¼å¼"""
        result = extractor.extract_document_number(input_text)
        assert result == expected
```

### é›†æˆæµ‹è¯•ç¤ºä¾‹

```python
# tests/integration/test_database_integration.py
import pytest
from crawler.database import MongoDBConnector
from crawler.data_models import PolicyDocument, DocumentLevel, TaxType

@pytest.fixture(scope="module")
def test_db():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    client = MongoClient("mongodb://localhost:27017")
    db = client["shared_cfo_test"]
    yield db
    # æ¸…ç†
    client.drop_database("shared_cfo_test")
    client.close()

def test_insert_and_retrieve_policy(test_db):
    """æµ‹è¯•æ’å…¥å’Œæ£€ç´¢æ”¿ç­–"""
    connector = MongoDBConnector(database="shared_cfo_test")

    policy = PolicyDocument(
        policy_id="TEST_2024_001",
        title="æµ‹è¯•æ”¿ç­–",
        source="æµ‹è¯•æ¥æº",
        url="https://test.example.com/policy/1",
        document_level=DocumentLevel.L3,
        tax_type=[TaxType.VAT],
        content="è¿™æ˜¯æµ‹è¯•å†…å®¹"
    )

    # æ’å…¥
    result = connector.insert_policy(policy)
    assert result is True

    # æ£€ç´¢
    retrieved = connector.get_policy_by_id("TEST_2024_001")
    assert retrieved is not None
    assert retrieved.title == "æµ‹è¯•æ”¿ç­–"
```

### æµ‹è¯•è¿è¡Œå‘½ä»¤

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/unit/

# è¿è¡Œé›†æˆæµ‹è¯•
pytest tests/integration/

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=crawler --cov=backend --cov-report=html

# æŸ¥çœ‹è¦†ç›–ç‡è¯¦æƒ…
open htmlcov/index.html
```

### æµ‹è¯•æœ€ä½³å®è·µ

1. **ä½¿ç”¨ fixture å¤ç”¨æµ‹è¯•æ•°æ®**
2. **Mock å¤–éƒ¨ä¾èµ–**ï¼ˆrequests, MongoDBï¼‰
3. **æµ‹è¯•è¾¹ç•Œæ¡ä»¶**ï¼ˆNone, ç©ºå­—ç¬¦ä¸², å¼‚å¸¸å€¼ï¼‰
4. **ä½¿ç”¨ parametrize å‡å°‘é‡å¤ä»£ç **

---

## å¼€å‘å·¥å…·é…ç½®

æœ¬ç« èŠ‚æä¾›ä»£ç è´¨é‡å·¥å…·çš„é…ç½®ï¼Œå¸®åŠ©ä¿æŒä»£ç é£æ ¼ä¸€è‡´ã€‚

### ä»£ç æ ¼å¼åŒ–ï¼šBlack

**å®‰è£…**ï¼š
```bash
pip install black
```

**é…ç½®æ–‡ä»¶**ï¼š`pyproject.toml`
```toml
[tool.black]
line-length = 100
target-version = ['py310']
include = '\.pyi?$'
extend-exclude = '''
/(
  \.eggs
  | \.git
  | \.venv
  | build
  | dist
)/
'''
```

**ä½¿ç”¨**ï¼š
```bash
# æ ¼å¼åŒ–æ•´ä¸ªé¡¹ç›®
black .

# æ£€æŸ¥æ ¼å¼ï¼ˆä¸ä¿®æ”¹æ–‡ä»¶ï¼‰
black --check .

# ä»…æ ¼å¼åŒ–ä¿®æ”¹çš„æ–‡ä»¶
black --diff .
```

### Lintingï¼šRuff

**å®‰è£…**ï¼š
```bash
pip install ruff
```

**é…ç½®æ–‡ä»¶**ï¼š`pyproject.toml`
```toml
[tool.ruff]
line-length = 100
target-version = "py310"

select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
    "UP",  # pyupgrade
]

ignore = [
    "E501",  # line too long (ç”± black å¤„ç†)
    "B008",  # do not perform function calls in argument defaults
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]
```

**ä½¿ç”¨**ï¼š
```bash
# æ£€æŸ¥ä»£ç 
ruff check .

# è‡ªåŠ¨ä¿®å¤é—®é¢˜
ruff check --fix .
```

### ç±»å‹æ£€æŸ¥ï¼šMypy

**å®‰è£…**ï¼š
```bash
pip install mypy
```

**é…ç½®æ–‡ä»¶**ï¼š`pyproject.toml`
```toml
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false
check_untyped_defs = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false

[[tool.mypy.overrides]]
module = "playwright.*,bs4.*"
ignore_missing_imports = true
```

**ä½¿ç”¨**ï¼š
```bash
# ç±»å‹æ£€æŸ¥
mypy crawler/
```

### Pre-commit Hooks

**å®‰è£…**ï¼š
```bash
pip install pre-commit
```

**é…ç½®æ–‡ä»¶**ï¼š`.pre-commit-config.yaml`
```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml

  - repo: https://github.com/psf/black
    rev: 24.1.1
    hooks:
      - id: black

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.15
    hooks:
      - id: ruff
        args: [--fix]
```

**å¯ç”¨**ï¼š
```bash
# å®‰è£… hooks
pre-commit install

# æ‰‹åŠ¨è¿è¡Œ
pre-commit run --all-files
```

---

## ç‰ˆæœ¬ç®¡ç†è§„èŒƒ

æœ¬ç« èŠ‚å®šä¹‰ä»£ç ç‰ˆæœ¬ç®¡ç†è§„èŒƒï¼Œè§£å†³ç‰ˆæœ¬æ··ä¹±é—®é¢˜ã€‚

### å½“å‰ç‰ˆæœ¬çŠ¶æ€

**å·²æ¸…ç†å®Œæˆï¼ˆ2026-01-29ï¼‰**ï¼šæ‰€æœ‰åºŸå¼ƒç‰ˆæœ¬å·²å½’æ¡£åˆ° `crawler/archive/` å’Œ `archive/` ç›®å½•ï¼Œç”Ÿäº§æ–‡ä»¶å·²é‡å‘½åå»æ‰ç‰ˆæœ¬å·åç¼€ã€‚

### æ–‡ä»¶å‘½åè§„åˆ™

```
æ¨¡å—å.py  # ç”Ÿäº§æ–‡ä»¶ï¼Œæ— ç‰ˆæœ¬å·åç¼€

âœ… å½“å‰ç”Ÿäº§æ–‡ä»¶ï¼š
crawler/base_crawler.py           # åŸºç¡€çˆ¬è™«æ¡†æ¶
crawler/chinatax_crawler.py       # ç¨åŠ¡æ€»å±€çˆ¬è™«
crawler/crawler_12366.py          # 12366çˆ¬è™«
crawler/data_models.py            # æ•°æ®æ¨¡å‹
crawler/database.py               # æ•°æ®åº“è¿æ¥
```

### å½’æ¡£æ–‡ä»¶

åºŸå¼ƒç‰ˆæœ¬å·²å®‰å…¨å½’æ¡£ï¼š
- `crawler/archive/` - åºŸå¼ƒçš„çˆ¬è™«ç‰ˆæœ¬æ–‡ä»¶
- `archive/` - æ ¹ç›®å½•åºŸå¼ƒæ–‡ä»¶ï¼ˆæµ‹è¯•è„šæœ¬ã€æ—§ç‰ˆæœ¬çˆ¬è™«ç­‰ï¼‰

### æ–°åŠŸèƒ½å¼€å‘æµç¨‹

1. åŸºäºå½“å‰ç”Ÿäº§ä»£ç å¼€å‘
2. åˆ›å»ºæ–°åˆ†æ”¯ï¼š`feature/åŠŸèƒ½å`
3. ç¼–å†™æµ‹è¯•ï¼ˆæ”¾åˆ° `tests/` ç›®å½•ï¼‰
4. å®ç°åŠŸèƒ½
5. ä»£ç å®¡æŸ¥
6. åˆå¹¶ä¸»åˆ†æ”¯

**æ³¨æ„**ï¼šæ–°å¼€å‘ä¸éœ€è¦æ·»åŠ ç‰ˆæœ¬å·åç¼€ï¼Œç›´æ¥åœ¨ä¸»æ–‡ä»¶ä¸Šä¿®æ”¹å³å¯ã€‚å¦‚éœ€å®éªŒæ€§åŠŸèƒ½ï¼Œä½¿ç”¨ç‹¬ç«‹çš„åˆ†æ”¯è€Œéç‰ˆæœ¬å·æ–‡ä»¶ã€‚

## AI ä»£ç ç”ŸæˆæŒ‡å—

æœ¬ç« èŠ‚ä¸“é—¨é’ˆå¯¹ AI æ¨¡å‹ï¼ˆglm-4.7ï¼‰ç”Ÿæˆä»£ç æ—¶çš„ä¼˜åŒ–å»ºè®®ã€‚

### ç”Ÿæˆä»£ç çš„æ ¸å¿ƒåŸåˆ™

**1. ç±»å‹æ³¨è§£ä¼˜å…ˆ**

```python
# âŒ AI ç»å¸¸ç”Ÿæˆçš„ä»£ç 
def process_policy(url, db):
    data = fetch_data(url)
    db.save(data)
    return data

# âœ… æœŸæœ›çš„ä»£ç 
from typing import Optional
from crawler.data_models import PolicyDocument
from crawler.database import MongoDBConnector

def process_policy(
    url: str,
    db: MongoDBConnector
) -> Optional[PolicyDocument]:
    """å¤„ç†æ”¿ç­–æ•°æ®

    Args:
        url: æ”¿ç­–URL
        db: æ•°æ®åº“è¿æ¥å™¨

    Returns:
        å¤„ç†åçš„æ”¿ç­–æ–‡æ¡£ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        data = fetch_data(url)
        db.insert_policy(data)
        return data
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {url}, {e}")
        return None
```

**2. ä½¿ç”¨é¡¹ç›®å·²å®šä¹‰çš„æ•°æ®æ¨¡å‹**

```python
# âœ… ä½¿ç”¨å·²æœ‰çš„ç±»å‹
from crawler.data_models import (
    PolicyDocument,      # ä½¿ç”¨è¿™ä¸ª
    DocumentLevel,       # ä½¿ç”¨è¿™ä¸ªï¼ˆä¸è¦ç”¨ strï¼‰
    TaxType,            # ä½¿ç”¨è¿™ä¸ªï¼ˆä¸è¦ç”¨ strï¼‰
    Region,             # ä½¿ç”¨è¿™ä¸ª
    DocumentType,       # ä½¿ç”¨è¿™ä¸ª
)

# âŒ ä¸è¦é‡æ–°å®šä¹‰
class Policy:  # âŒ å·²æœ‰ PolicyDocument
    pass

# âŒ ä¸è¦ä½¿ç”¨å­—ç¬¦ä¸²
def get_level(level: str):  # âŒ åº”è¯¥ç”¨ DocumentLevel
    pass
```

### çˆ¬è™«å‡½æ•°æ¨¡æ¿

```python
from crawler.exceptions import CrawlerTimeoutError, CrawlerParseError
from crawler.data_models import PolicyDocument
import logging

logger = logging.getLogger(__name__)

def crawl_policy_page(url: str) -> Optional[PolicyDocument]:
    """çˆ¬å–æ”¿ç­–é¡µé¢

    Args:
        url: æ”¿ç­–URL

    Returns:
        æ”¿ç­–æ–‡æ¡£å¯¹è±¡ï¼Œå¤±è´¥è¿”å›None

    Raises:
        CrawlerTimeoutError: è¯·æ±‚è¶…æ—¶
        CrawlerParseError: é¡µé¢è§£æå¤±è´¥
    """
    try:
        # 1. è·å–é¡µé¢
        from requests.exceptions import Timeout
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        # 2. è§£ææ•°æ®
        policy_data = parse_html(response.text)
        if not policy_data:
            raise CrawlerParseError(f"æ— æ³•è§£æé¡µé¢: {url}")

        # 3. æ„å»ºå¯¹è±¡
        policy = PolicyDocument(**policy_data)
        logger.info(f"æˆåŠŸçˆ¬å–: {policy.policy_id} - {policy.title}")

        return policy

    except Timeout as e:
        logger.error(f"è¯·æ±‚è¶…æ—¶: {url}")
        raise CrawlerTimeoutError(f"è¯·æ±‚è¶…æ—¶: {url}") from e

    except requests.RequestException as e:
        logger.error(f"è¯·æ±‚å¤±è´¥: {url}, çŠ¶æ€ç : {e.response.status_code if e.response else 'N/A'}")
        return None
```

### æ•°æ®åº“æ“ä½œæ¨¡æ¿

```python
from crawler.database import MongoDBConnector
from crawler.data_models import PolicyDocument

def save_policy_batch(
    policies: list[PolicyDocument],
    db: MongoDBConnector
) -> dict[str, int]:
    """æ‰¹é‡ä¿å­˜æ”¿ç­–

    Args:
        policies: æ”¿ç­–åˆ—è¡¨
        db: æ•°æ®åº“è¿æ¥å™¨

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {'success': int, 'failed': int, 'duplicate': int}
    """
    stats = {'success': 0, 'failed': 0, 'duplicate': 0}

    for policy in policies:
        try:
            result = db.insert_policy(policy)
            if result:
                stats['success'] += 1
            else:
                stats['duplicate'] += 1

        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {policy.policy_id}, {e}")
            stats['failed'] += 1

    logger.info(f"æ‰¹é‡ä¿å­˜å®Œæˆ: {stats}")
    return stats
```

### æ—¥å¿—è®°å½•æ¨¡æ¿

```python
# ç»“æ„åŒ–æ—¥å¿—çš„æ ‡å‡†æ ¼å¼
logger.info(f"æ“ä½œå¼€å§‹ - å‚æ•°: {param1}, {param2}")
logger.debug(f"è°ƒè¯•ä¿¡æ¯ - å˜é‡å€¼: {variable}")
logger.warning(f"è­¦å‘Šä¿¡æ¯ - è·³è¿‡: {item}, åŸå› : {reason}")
logger.error(f"é”™è¯¯ä¿¡æ¯ - æ“ä½œå¤±è´¥: {operation}, é”™è¯¯: {error}")
logger.critical(f"ä¸¥é‡é”™è¯¯ - ç³»ç»Ÿæ•…éšœ: {system_error}")

# âŒ ä¸è¦ä½¿ç”¨
logger.info("å¼€å§‹")  # ç¼ºå°‘ä¸Šä¸‹æ–‡
logger.error("å¤±è´¥")  # ç¼ºå°‘å…·ä½“ä¿¡æ¯
```

### æµ‹è¯•ä»£ç æ¨¡æ¿

```python
import pytest
from crawler.base_crawler import FieldExtractor

class TestFieldExtractor:
    """å­—æ®µæå–å™¨æµ‹è¯•"""

    @pytest.fixture
    def extractor(self):
        """æµ‹è¯• fixture"""
        return FieldExtractor()

    def test_extract_success(self, extractor):
        """æµ‹è¯•æˆåŠŸæå–"""
        # Arrange
        text = "æµ‹è¯•æ–‡æœ¬"
        expected = "æœŸæœ›ç»“æœ"

        # Act
        result = extractor.extract(text)

        # Assert
        assert result == expected

    @pytest.mark.parametrize("input,expected", [
        ("case1", "result1"),
        ("case2", "result2"),
    ])
    def test_multiple_cases(self, extractor, input, expected):
        """æµ‹è¯•å¤šä¸ªæ¡ˆä¾‹"""
        result = extractor.extract(input)
        assert result == expected
```

### ä»£ç ç”Ÿæˆæ£€æŸ¥æ¸…å•

ç”Ÿæˆä»£ç åï¼Œè¯·æ£€æŸ¥ï¼š

- [ ] æ‰€æœ‰å‡½æ•°éƒ½æœ‰ç±»å‹æ³¨è§£ï¼ˆå‚æ•°å’Œè¿”å›å€¼ï¼‰
- [ ] ä½¿ç”¨äº†é¡¹ç›®å·²æœ‰çš„æ•°æ®æ¨¡å‹ï¼ˆ`data_models_v2.py`ï¼‰
- [ ] ä½¿ç”¨äº†å…·ä½“çš„å¼‚å¸¸ç±»å‹ï¼ˆä¸æ˜¯ `except Exception`ï¼‰
- [ ] åŒ…å«è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
- [ ] å‡½æ•°æœ‰ docstringï¼ˆè¯´æ˜å‚æ•°ã€è¿”å›å€¼ã€å¼‚å¸¸ï¼‰
- [ ] å¯¼å…¥äº†æ­£ç¡®çš„æ¨¡å—ï¼ˆä½¿ç”¨ `from ... import ...`ï¼‰
- [ ] ç¬¦åˆé¡¹ç›®å‘½åè§„èŒƒï¼ˆsnake_case å‡½æ•°ï¼ŒPascalCase ç±»ï¼‰
- [ ] éµå¾ªé¡¹ç›®ç»“æ„ï¼ˆç»§æ‰¿ `BaseCrawler`ï¼Œä½¿ç”¨ `MongoDBConnector`ï¼‰

### å¿«é€Ÿå‚è€ƒï¼šå¸¸ç”¨å¯¼å…¥

```python
# æ•°æ®æ¨¡å‹
from crawler.data_models import (
    PolicyDocument, DocumentLevel, TaxType, Region,
    DocumentType, ValidityStatus, TaxCategory
)

# æ•°æ®åº“
from crawler.database import MongoDBConnector

# çˆ¬è™«åŸºç±»
from crawler.base_crawler import BaseCrawler, FieldExtractor, ComplianceChecker

# å¼‚å¸¸
from crawler.exceptions import (
    CrawlerError, CrawlerTimeoutError,
    CrawlerParseError, CrawlerComplianceError
)

# ç±»å‹æç¤º
from typing import List, Dict, Optional, Tuple, Any

# æ—¥å¿—
import logging
logger = logging.getLogger(__name__)

# HTTP è¯·æ±‚
import requests
from requests.exceptions import RequestException, Timeout

# HTML è§£æ
from bs4 import BeautifulSoup
```

---

## éƒ¨ç½²ä¿¡æ¯

### é˜¿é‡Œäº‘ ECS éƒ¨ç½²

**æœåŠ¡å™¨**: 120.78.5.4

| é¡¹ç›® | ä¿¡æ¯ |
|------|------|
| **éƒ¨ç½²è·¯å¾„** | `/opt/shared_cfo/` |
| **MongoDB** | 7.0ï¼Œæ— è®¤è¯æ¨¡å¼ |
| **Python** | 3.10.12 |
| **æµè§ˆå™¨** | Chromium + xvfb è™šæ‹Ÿæ˜¾ç¤º |

### éƒ¨ç½²è„šæœ¬

- `deploy_fresh_ecs.sh` - å…¨æ–°éƒ¨ç½²è„šæœ¬
- `DEPLOYMENT_SUMMARY.md` - éƒ¨ç½²æ€»ç»“æ–‡æ¡£

### å¥åº·æ£€æŸ¥

```bash
# åç«¯å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health

# æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
python run_crawler.py status
```

---

## å·¥å…·é›†

### CLI å·¥å…· (tools/ ç›®å½•)

| å·¥å…· | åŠŸèƒ½ |
|------|------|
| `tools/policy_query.py` | æ”¿ç­–æŸ¥è¯¢å·¥å…· |
| `tools/crawler_monitor.py` | çˆ¬è™«ç›‘æ§é¢æ¿ |
| `tools/web_tool.py` | Flask Web å·¥å…· |

#### policy_query.py å‘½ä»¤

```bash
python tools/policy_query.py stats                  # æ•°æ®ç»Ÿè®¡
python tools/policy_query.py search "å¢å€¼ç¨" -i     # æœç´¢æ”¿ç­–
python tools/policy_query.py list --limit 20        # æœ€è¿‘æ”¿ç­–
python tools/policy_query.py view <policy_id>       # æŸ¥çœ‹è¯¦æƒ…
python tools/policy_query.py export "å…³é”®è¯" -o file.md  # å¯¼å‡ºæ•°æ®
```

#### crawler_monitor.py å‘½ä»¤

```bash
python tools/crawler_monitor.py monitor --hours 24   # ç›‘æ§çŠ¶æ€
python tools/crawler_monitor.py watch               # å®æ—¶ç›‘æ§
python tools/crawler_monitor.py status              # æœåŠ¡çŠ¶æ€
```

### Web å·¥å…·

**æ–‡ä»¶**: `tools/web_tool.py`, `templates/index.html`

**å¯åŠ¨æ–¹å¼**:
```bash
# æœ¬åœ°å¼€å‘
python tools/web_tool.py               # è®¿é—® http://localhost:5000

# ç”Ÿäº§ç¯å¢ƒ (ECS)
cd /opt/shared_cfo
nohup python3 tools/web_tool.py > logs/web_tool.log 2>&1 &
```

**Web åŠŸèƒ½**:
- ğŸ“Š å¸¦å¯è§†åŒ–å›¾è¡¨çš„æ•°æ®ç»Ÿè®¡
- ğŸ” æ”¿ç­–æœç´¢ï¼ˆå«å±‚çº§ã€æ¥æºç­›é€‰ï¼‰
- ğŸ“‹ æœ€è¿‘æ”¿ç­–åˆ—è¡¨
- ğŸ“ˆ çˆ¬è™«ç›‘æ§é¢æ¿
- ğŸ“„ æ”¿ç­–è¯¦æƒ…æŸ¥çœ‹
- ğŸ’¾ å¯¼å‡ºä¸º Markdown

**SSH éš§é“ï¼ˆè¿œç¨‹è®¿é—®ï¼‰**:
```bash
ssh -i ~/.ssh/id_ed25519 -L 5000:localhost:5000 root@120.78.5.4
# ç„¶åè®¿é—® http://localhost:5000
```

### REST API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| `/` | GET | Web ç•Œé¢ |
| `/api/stats` | GET | æ•°æ®ç»Ÿè®¡ |
| `/api/search` | GET | æœç´¢æ”¿ç­– |
| `/api/policy/<id>` | GET | æ”¿ç­–è¯¦æƒ… |
| `/api/recent` | GET | æœ€è¿‘æ”¿ç­– |
| `/api/monitor` | GET | çˆ¬è™«ç›‘æ§æ•°æ® |
| `/api/export` | GET | å¯¼å‡º Markdown |

---

*æœ€åæ›´æ–°: 2026-01-29*
