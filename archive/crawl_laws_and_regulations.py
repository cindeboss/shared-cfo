#!/usr/bin/env python3
"""
å…±äº«CFO - æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„çˆ¬è™«
çˆ¬å–å®žä½“æ³•ã€ç¨‹åºæ³•åŠç›¸å…³è¡Œæ”¿æ³•è§„
"""

import asyncio
import re
import os
from datetime import datetime
from pymongo import MongoClient
from playwright.async_api import async_playwright

# MongoDBé…ç½®
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://localhost:27017/')
MONGO_DB = 'shared_cfo'
MONGO_COLLECTION = 'policies'

# ç›®æ ‡æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„åˆ—è¡¨
TARGET_LAWS = {
    # === å®žä½“æ³• ===
    'å¢žå€¼ç¨Žæ³•': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20241225165742856',
        'level': 'L1',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['å¢žå€¼ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½å¢žå€¼ç¨Žæ³•'
    },
    'ä¸ªäººæ‰€å¾—ç¨Žæ³•': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20180831192857515',
        'level': 'L1',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨Žæ³•'
    },
    'ä¼ä¸šæ‰€å¾—ç¨Žæ³•': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20070316173633701',
        'level': 'L1',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨Žæ³•'
    },
    'ç¨Žæ”¶å¾æ”¶ç®¡ç†æ³•': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20150427142125701',
        'level': 'L1',
        'category': 'ç¨‹åºç¨Ž',
        'tax_type': ['ç¨Žæ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ç¨Žæ”¶å¾æ”¶ç®¡ç†æ³•'
    },

    # === è¡Œæ”¿æ³•è§„ï¼ˆå®žæ–½æ¡ä¾‹ï¼‰ ===
    'å¢žå€¼ç¨Žæ³•å®žæ–½æ¡ä¾‹': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20171030182632700',
        'level': 'L2',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['å¢žå€¼ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½å¢žå€¼ç¨Žæš‚è¡Œæ¡ä¾‹'
    },
    'ä¸ªäººæ‰€å¾—ç¨Žæ³•å®žæ–½æ¡ä¾‹': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20181220173530700',
        'level': 'L2',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨Žæ³•å®žæ–½æ¡ä¾‹'
    },
    'ä¼ä¸šæ‰€å¾—ç¨Žæ³•å®žæ–½æ¡ä¾‹': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20071130152133700',
        'level': 'L2',
        'category': 'å®žä½“ç¨Ž',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨Ž'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨Žæ³•å®žæ–½æ¡ä¾‹'
    },
    'ç¨Žæ”¶å¾æ”¶ç®¡ç†æ³•å®žæ–½ç»†åˆ™': {
        'url': 'https://fgk.chinatax.gov.cn/api/rest/v1/zcfgk/detail?code=20160210165243700',
        'level': 'L2',
        'category': 'ç¨‹åºç¨Ž',
        'tax_type': ['ç¨Žæ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åŽäººæ°‘å…±å’Œå›½ç¨Žæ”¶å¾æ”¶ç®¡ç†æ³•å®žæ–½ç»†åˆ™'
    },
}

# å¤‡ç”¨é¡µé¢URLï¼ˆå¦‚æžœAPIä¸å¯ç”¨ï¼‰
FALLBACK_URLS = {
    'å¢žå€¼ç¨Žæ³•': 'https://www.npc.gov.cn/npc/c234/20241225a5a9a09.shtml',
    'ä¸ªäººæ‰€å¾—ç¨Žæ³•': 'https://www.npc.gov.cn/npc/c234/20180831a48f9d9.shtml',
    'ä¼ä¸šæ‰€å¾—ç¨Žæ³•': 'https://www.npc.gov.cn/npc/c234/20070316a0e510e.shtml',
    'ç¨Žæ”¶å¾æ”¶ç®¡ç†æ³•': 'https://www.npc.gov.cn/npc/c234/20150427a4c7c2e.shtml',
}


class LawsCrawler:
    """æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„çˆ¬è™«"""

    def __init__(self):
        self.client = MongoClient(MONGO_URI)
        self.db = self.client[MONGO_DB]
        self.collection = self.db[MONGO_COLLECTION]
        self.results = []

    def extract_fields(self, title, content, url):
        """æå–æ”¿ç­–å­—æ®µ"""
        # æå–å‘æ–‡å­—å·
        document_number = None
        patterns = [
            r'ä¸»å¸­ä»¤[ç¬¬ç¬¬](\d+)å·',
            r'å›½åŠ¡é™¢ä»¤[ç¬¬ç¬¬](\d+)å·',
            r'(è´¢ç¨Ž|ç¨Žæ€»)[\u3000\s]{0,5}[ã€”\(]\d{4}[ã€•\)]\d{1,3}å·',
            r'å›½å®¶ç¨ŽåŠ¡æ€»å±€å…¬å‘Š\d{4}å¹´ç¬¬\d+å·',
        ]
        for pattern in patterns:
            match = re.search(pattern, content[:500])
            if match:
                document_number = match.group(0)
                break

        # æå–å‘å¸ƒæ—¥æœŸ
        publish_date = None
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
        ]
        for pattern in date_patterns:
            match = re.search(pattern, content[:1000])
            if match:
                publish_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                break

        # æå–ç”Ÿæ•ˆæ—¥æœŸ
        effective_date = None
        eff_patterns = [
            r'è‡ª(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥èµ·æ–½è¡Œ',
            r'è‡ª(\d{4})-(\d{1,2})-(\d{1,2})èµ·æ–½è¡Œ',
        ]
        for pattern in eff_patterns:
            match = re.search(pattern, content)
            if match:
                effective_date = f"{match.group(1)}-{match.group(2).zfill(2)}-{match.group(3).zfill(2)}"
                break

        return {
            'document_number': document_number,
            'publish_date': publish_date,
            'effective_date': effective_date,
        }

    async def crawl_from_api(self, name, info, browser):
        """ä»ŽAPIçˆ¬å–"""
        try:
            page = await browser.new_page()
            await page.goto(info['url'], wait_until='networkidle', timeout=30000)

            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(2)

            content = await page.content()

            # æå–æ­£æ–‡å†…å®¹
            title = info['expected_title']

            # å°è¯•å¤šç§æ–¹å¼æå–æ­£æ–‡
            body_content = ''
            selectors = [
                '.content-body',
                '.article-content',
                '.detail-content',
                'body',
            ]

            for selector in selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        body_content = await element.inner_text()
                        if len(body_content) > 500:
                            break
                except:
                    continue

            if not body_content or len(body_content) < 100:
                body_content = await page.inner_text('body')

            await page.close()

            if len(body_content) < 100:
                print(f"  âš ï¸  {name}: å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
                return None

            # æå–å­—æ®µ
            fields = self.extract_fields(title, body_content, info['url'])

            # ç”Ÿæˆpolicy_id
            policy_id = f"LAW_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': title,
                'source': 'å›½å®¶ç¨ŽåŠ¡æ€»å±€' if 'chinatax.gov.cn' in info['url'] else 'å…¨å›½äººå¤§',
                'url': info['url'],
                'content': body_content,
                'document_level': info['level'],
                'document_type': 'æ³•å¾‹' if info['level'] == 'L1' else 'è¡Œæ”¿æ³•è§„',
                'tax_category': info['category'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'publish_date': fields['publish_date'],
                'document_number': fields['document_number'],
                'effective_date': fields['effective_date'],
                'crawled_at': datetime.now(),
                'quality_score': 5,  # æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„è´¨é‡æœ€é«˜
            }

            return policy_data

        except Exception as e:
            print(f"  âŒ APIçˆ¬å–å¤±è´¥ {name}: {e}")
            return None

    async def crawl_from_npc(self, name, url, info, browser):
        """ä»Žå…¨å›½äººå¤§å®˜ç½‘çˆ¬å–"""
        try:
            page = await browser.new_page()
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)

            # äººå¤§å®˜ç½‘æ­£æ–‡æå–
            body_content = ''
            selectors = [
                '.zwx3-box',
                '.content',
                'article',
            ]

            for selector in selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        body_content = await element.inner_text()
                        if len(body_content) > 500:
                            break
                except:
                    continue

            await page.close()

            if not body_content or len(body_content) < 100:
                return None

            title = info['expected_title']
            fields = self.extract_fields(title, body_content, url)
            policy_id = f"LAW_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': title,
                'source': 'å…¨å›½äººå¤§',
                'url': url,
                'content': body_content,
                'document_level': info['level'],
                'document_type': 'æ³•å¾‹',
                'tax_category': info['category'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'publish_date': fields['publish_date'],
                'document_number': fields['document_number'],
                'effective_date': fields['effective_date'],
                'crawled_at': datetime.now(),
                'quality_score': 5,
            }

            return policy_data

        except Exception as e:
            print(f"  âŒ äººå¤§å®˜ç½‘çˆ¬å–å¤±è´¥ {name}: {e}")
            return None

    async def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰ç›®æ ‡æ³•å¾‹"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)

            print("=" * 60)
            print("å¼€å§‹çˆ¬å–æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„")
            print("=" * 60)

            for name, info in TARGET_LAWS.items():
                print(f"\nðŸ“‹ æ­£åœ¨çˆ¬å–: {name}")

                # é¦–å…ˆå°è¯•API
                policy = await self.crawl_from_api(name, info, browser)

                # å¦‚æžœAPIå¤±è´¥ï¼Œå°è¯•äººå¤§å®˜ç½‘
                if not policy and name in FALLBACK_URLS:
                    print(f"  ðŸ”„ å°è¯•å¤‡ç”¨æº: å…¨å›½äººå¤§å®˜ç½‘")
                    policy = await self.crawl_from_npc(name, FALLBACK_URLS[name], info, browser)

                if policy:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = self.collection.find_one({'policy_id': policy['policy_id']})
                    if existing:
                        print(f"  â„¹ï¸  å·²å­˜åœ¨ï¼Œæ›´æ–°æ•°æ®")
                        self.collection.update_one(
                            {'policy_id': policy['policy_id']},
                            {'$set': policy}
                        )
                    else:
                        self.collection.insert_one(policy)
                        print(f"  âœ… ä¿å­˜æˆåŠŸ")

                    self.results.append({
                        'name': name,
                        'title': policy['title'],
                        'level': policy['document_level'],
                        'content_length': len(policy['content']),
                    })
                else:
                    print(f"  âš ï¸  {name}: çˆ¬å–å¤±è´¥")

            await browser.close()

        print("\n" + "=" * 60)
        print("çˆ¬å–å®Œæˆ!")
        print("=" * 60)

    def print_results(self):
        """æ‰“å°ç»“æžœç»Ÿè®¡"""
        print(f"\nðŸ“Š çˆ¬å–ç»“æžœ:")
        print(f"æˆåŠŸ: {len(self.results)} æ¡")

        for r in self.results:
            print(f"  â€¢ [{r['level']}] {r['name']}")
            print(f"    æ ‡é¢˜: {r['title'][:50]}...")
            print(f"    å†…å®¹é•¿åº¦: {r['content_length']} å­—ç¬¦")


async def main():
    """ä¸»å‡½æ•°"""
    crawler = LawsCrawler()
    await crawler.crawl_all()
    crawler.print_results()


if __name__ == '__main__':
    asyncio.run(main())
