#!/usr/bin/env python3
"""
å¤šæºæ³•å¾‹å’Œè¡Œæ”¿æ³•è§„çˆ¬è™« v2
ä½¿ç”¨Playwright + requestsæ··åˆæ¨¡å¼
"""

import asyncio
import re
import logging
import requests
from datetime import datetime
from pymongo import MongoClient
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# MongoDBé…ç½®
MONGO_URI = 'mongodb://localhost:27017/'
MONGO_DB = 'shared_cfo'
MONGO_COLLECTION = 'policies'

# ç›®æ ‡æ³•å¾‹
TARGET_LAWS = {
    'å¢å€¼ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20241225a5a9a09.shtml',
        'fallback_url': 'https://www.npc.gov.cn/npc/c2/20241225a5a9a09.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'tax_type': ['å¢å€¼ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½å¢å€¼ç¨æ³•',
    },
    'ä¸ªäººæ‰€å¾—ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20180831a48f9d9.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨æ³•',
    },
    'ä¼ä¸šæ‰€å¾—ç¨æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20070316a0e510e.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•',
    },
    'ç¨æ”¶å¾æ”¶ç®¡ç†æ³•': {
        'npc_url': 'https://www.npc.gov.cn/npc/c234/20150427a4c7c2e.shtml',
        'level': 'L1',
        'type': 'æ³•å¾‹',
        'tax_type': ['ç¨æ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•',
    },
}

# ç›®æ ‡è¡Œæ”¿æ³•è§„
TARGET_REGULATIONS = {
    'å¢å€¼ç¨æš‚è¡Œæ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2017-12/29/content_5343642.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'tax_type': ['å¢å€¼ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½å¢å€¼ç¨æš‚è¡Œæ¡ä¾‹',
    },
    'ä¸ªäººæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2018-12/22/content_5350262.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'tax_type': ['ä¸ªäººæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¸ªäººæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹',
    },
    'ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2007-12/11/content_5279817.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'tax_type': ['ä¼ä¸šæ‰€å¾—ç¨'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹',
    },
    'ç¨æ”¶å¾æ”¶ç®¡ç†æ³•å®æ–½ç»†åˆ™': {
        'gov_url': 'https://www.gov.cn/zhengce/content/2016-02/06/content_5031145.htm',
        'level': 'L2',
        'type': 'è¡Œæ”¿æ³•è§„',
        'tax_type': ['ç¨æ”¶å¾ç®¡'],
        'expected_title': 'ä¸­åäººæ°‘å…±å’Œå›½ç¨æ”¶å¾æ”¶ç®¡ç†æ³•å®æ–½ç»†åˆ™',
    },
}


class MultiSourceLawsCrawlerV2:
    """å¤šæºæ³•å¾‹çˆ¬è™« V2"""

    def __init__(self):
        self.client = MongoClient(MONGO_URI)
        self.db = self.client[MONGO_DB]
        self.collection = self.db[MONGO_COLLECTION]
        self.results = []

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)

        # requests session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        })

    def fetch_with_requests(self, url):
        """ä½¿ç”¨requestsè·å–é¡µé¢"""
        try:
            resp = self.session.get(url, timeout=30, verify=False)  # å¿½ç•¥SSLéªŒè¯
            resp.encoding = resp.apparent_encoding
            return resp.text
        except Exception as e:
            self.logger.error(f"requestsè·å–å¤±è´¥: {e}")
            return None

    def extract_npc_content(self, html, title):
        """æå–äººå¤§å®˜ç½‘æ­£æ–‡"""
        soup = BeautifulSoup(html, 'html.parser')

        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            ('div', {'class': 'zwx3-box'}),
            ('div', {'class': 'content'}),
            ('div', {'id': 'content'}),
            ('article', {}),
            ('div', {'class': 'text'}),
        ]

        for tag, attrs in selectors:
            elem = soup.find(tag, attrs)
            if elem:
                text = elem.get_text(separator='\n', strip=True)
                if len(text) > 500:
                    return text

        # å¤‡ç”¨ï¼šè·å–æ•´ä¸ªbody
        return soup.get_text(separator='\n', strip=True)

    def extract_gov_content(self, html, title):
        """æå–å›½åŠ¡é™¢å®˜ç½‘æ­£æ–‡"""
        soup = BeautifulSoup(html, 'html.parser')

        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            ('div', {'class': 'content-text'}),
            ('div', {'class': 'article-content'}),
            ('div', {'id': 'content'}),
            ('div', {'class': 'text'}),
            ('article', {}),
        ]

        for tag, attrs in selectors:
            elem = soup.find(tag, attrs)
            if elem:
                text = elem.get_text(separator='\n', strip=True)
                if len(text) > 500:
                    return text

        # å°è¯•æ‰¾åŒ…å«"ç¬¬ä¸€æ¡"çš„div
        for div in soup.find_all('div'):
            text = div.get_text(separator='\n', strip=True)
            if 'ç¬¬ä¸€æ¡' in text and len(text) > 500:
                return text

        return soup.get_text(separator='\n', strip=True)

    def crawl_npc_law_requests(self, name, info):
        """ä½¿ç”¨requestsä»äººå¤§å®˜ç½‘çˆ¬å–æ³•å¾‹"""
        try:
            url = info['npc_url']
            self.logger.info(f"[äººå¤§/requests] çˆ¬å–: {name}")

            html = self.fetch_with_requests(url)
            if not html:
                return None

            content = self.extract_npc_content(html, info['expected_title'])

            if len(content) < 200:
                self.logger.warning(f"[äººå¤§] {name}: å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                return None

            policy_id = f"NPC_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': info['expected_title'],
                'source': 'å…¨å›½äººå¤§',
                'url': url,
                'content': content,
                'document_level': info['level'],
                'document_type': info['type'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'crawled_at': datetime.now(),
                'quality_score': 5,
                'crawl_source': 'npc',
            }

            self.logger.info(f"[äººå¤§] {name}: æˆåŠŸ ({len(content)} å­—ç¬¦)")
            return policy_data

        except Exception as e:
            self.logger.error(f"[äººå¤§] {name}: å¤±è´¥ - {e}")
            return None

    async def crawl_gov_regulation_playwright(self, name, info, browser):
        """ä½¿ç”¨Playwrightä»å›½åŠ¡é™¢å®˜ç½‘çˆ¬å–è¡Œæ”¿æ³•è§„"""
        try:
            url = info['gov_url']
            self.logger.info(f"[å›½åŠ¡é™¢] çˆ¬å–: {name}")

            page = await browser.new_page()
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)

            # è·å–HTML
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')

            # æå–æ­£æ–‡
            content = self.extract_gov_content(html, info['expected_title'])

            await page.close()

            if len(content) < 200:
                self.logger.warning(f"[å›½åŠ¡é™¢] {name}: å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦)")
                return None

            policy_id = f"GOV_{info['level']}_{name}_{datetime.now().strftime('%Y%m%d')}"

            policy_data = {
                'policy_id': policy_id,
                'title': info['expected_title'],
                'source': 'å›½åŠ¡é™¢',
                'url': url,
                'content': content,
                'document_level': info['level'],
                'document_type': info['type'],
                'tax_type': info['tax_type'],
                'region': 'å…¨å›½',
                'crawled_at': datetime.now(),
                'quality_score': 5,
                'crawl_source': 'gov',
            }

            self.logger.info(f"[å›½åŠ¡é™¢] {name}: æˆåŠŸ ({len(content)} å­—ç¬¦)")
            return policy_data

        except Exception as e:
            self.logger.error(f"[å›½åŠ¡é™¢] {name}: å¤±è´¥ - {e}")
            return None

    def save_policy(self, policy_data):
        """ä¿å­˜æ”¿ç­–åˆ°æ•°æ®åº“"""
        self.collection.delete_many({'title': policy_data['title']})
        self.collection.insert_one(policy_data)

    async def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰ç›®æ ‡æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹çˆ¬å–æ³•å¾‹å’Œè¡Œæ”¿æ³•è§„ (V2)")
        self.logger.info("=" * 60)

        # çˆ¬å–æ³•å¾‹ï¼ˆä½¿ç”¨requestsï¼‰
        self.logger.info("\n>>> ç¬¬ä¸€éƒ¨åˆ†ï¼šçˆ¬å–æ³•å¾‹ï¼ˆäººå¤§å®˜ç½‘ + requestsï¼‰")
        for name, info in TARGET_LAWS.items():
            policy = self.crawl_npc_law_requests(name, info)
            if policy:
                self.save_policy(policy)
                self.results.append({
                    'name': name,
                    'title': policy['title'],
                    'source': 'å…¨å›½äººå¤§',
                    'level': policy['document_level'],
                    'content_length': len(policy['content']),
                })

        # çˆ¬å–è¡Œæ”¿æ³•è§„ï¼ˆä½¿ç”¨Playwrightï¼‰
        self.logger.info("\n>>> ç¬¬äºŒéƒ¨åˆ†ï¼šçˆ¬å–è¡Œæ”¿æ³•è§„ï¼ˆå›½åŠ¡é™¢å®˜ç½‘ + Playwrightï¼‰")
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            for name, info in TARGET_REGULATIONS.items():
                policy = await self.crawl_gov_regulation_playwright(name, info, browser)
                if policy:
                    self.save_policy(policy)
                    self.results.append({
                        'name': name,
                        'title': policy['title'],
                        'source': 'å›½åŠ¡é™¢',
                        'level': policy['document_level'],
                        'content_length': len(policy['content']),
                    })
                await asyncio.sleep(1)
            await browser.close()

        self.logger.info("\n" + "=" * 60)
        self.logger.info("çˆ¬å–å®Œæˆ!")
        self.logger.info("=" * 60)

    def print_results(self):
        """æ‰“å°ç»“æœç»Ÿè®¡"""
        print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"æˆåŠŸ: {len(self.results)} æ¡\n")

        for r in self.results:
            print(f"  â€¢ [{r['level']}] {r['name']}")
            print(f"    æ¥æº: {r['source']}")
            print(f"    å†…å®¹é•¿åº¦: {r['content_length']} å­—ç¬¦")


async def main():
    """ä¸»å‡½æ•°"""
    crawler = MultiSourceLawsCrawlerV2()
    await crawler.crawl_all()
    crawler.print_results()


if __name__ == '__main__':
    import warnings
    warnings.filterwarnings('ignore', message='Unverified HTTPS request')
    asyncio.run(main())
