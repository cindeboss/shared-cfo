#!/usr/bin/env python3
"""
å…±äº«CFO - çˆ¬è™«ç›‘æ§å·¥å…·

å®æ—¶ç›‘æ§çˆ¬è™«è¿è¡ŒçŠ¶æ€ï¼ŒåŠæ—¶å‘ç°å’Œçº æ­£é—®é¢˜
"""

from pymongo import MongoClient
import argparse
import sys
import time
from datetime import datetime, timedelta
from collections import defaultdict
import subprocess


# MongoDB é…ç½®
MONGO_CONFIG = {
    'host': 'localhost',  # Use localhost for both local and server execution
    'port': 27017,
    'database': 'shared_cfo',
    'collection': 'policies',
}


class CrawlerMonitor:
    """çˆ¬è™«ç›‘æ§å™¨"""

    def __init__(self, config):
        self.config = config
        self.client = None
        self.db = None
        self.collection = None

    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            uri = f"mongodb://{self.config['host']}:{self.config['port']}"
            self.client = MongoClient(uri, serverSelectionTimeoutMS=5000)
            self.db = self.client[self.config['database']]
            self.collection = self.db[self.config['collection']]
            self.client.admin.command('ping')
            return True
        except Exception as e:
            print(f"âŒ è¿æ¥å¤±è´¥: {e}")
            return False

    def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        if self.client:
            self.client.close()

    def get_crawl_stats(self, hours: int = 24) -> dict:
        """è·å–çˆ¬å–ç»Ÿè®¡"""
        since = datetime.now() - timedelta(hours=hours)

        # æœ€è¿‘çˆ¬å–çš„æ•°æ®
        recent_policies = list(self.collection.find({'crawled_at': {'$gte': since.isoformat()}}))

        # æŒ‰æ—¶é—´åˆ†ç»„ç»Ÿè®¡
        hourly_stats = defaultdict(int)
        for policy in recent_policies:
            crawled_at = policy.get('crawled_at', '')
            if crawled_at:
                try:
                    dt = datetime.fromisoformat(crawled_at)
                    hour_key = dt.strftime('%Y-%m-%d %H:00')
                    hourly_stats[hour_key] += 1
                except:
                    pass

        # æŒ‰æ¥æºç»Ÿè®¡
        source_stats = defaultdict(int)
        for policy in recent_policies:
            source_stats[policy.get('source', 'æœªçŸ¥')] += 1

        # æŒ‰å±‚çº§ç»Ÿè®¡
        level_stats = defaultdict(int)
        for policy in recent_policies:
            level_stats[policy.get('document_level', 'æœªçŸ¥')] += 1

        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality_issues = {
            'missing_title': self.collection.count_documents({'title': {'$exists': False}}),
            'missing_url': self.collection.count_documents({'url': {'$exists': False}}),
            'missing_level': self.collection.count_documents({'document_level': {'$exists': False}}),
            'missing_content': self.collection.count_documents({
                'content': {'$exists': False},
                'crawled_at': {'$gte': since.isoformat()}  # åªæ£€æŸ¥æœ€è¿‘çš„æ•°æ®
            }),
        }

        return {
            'period_hours': hours,
            'total_recent': len(recent_policies),
            'hourly_stats': dict(hourly_stats),
            'source_stats': dict(source_stats),
            'level_stats': dict(level_stats),
            'quality_issues': quality_issues,
            'total_db': self.collection.count_documents({}),
        }

    def get_error_logs(self, hours: int = 24) -> list:
        """ä»æ—¥å¿—æ–‡ä»¶è·å–é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰"""
        # è¿™ä¸ªéœ€è¦è¯»å–æ—¥å¿—æ–‡ä»¶ï¼Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return []

    def check_data_quality(self) -> dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        total = self.collection.count_documents({})

        checks = {
            'completeness': {
                'title': self.collection.count_documents({'title': {'$exists': True, '$ne': ''}}),
                'url': self.collection.count_documents({'url': {'$exists': True, '$ne': ''}}),
                'source': self.collection.count_documents({'source': {'$exists': True, '$ne': ''}}),
                'document_level': self.collection.count_documents({'document_level': {'$exists': True, '$ne': ''}}),
            },
            'uniqueness': {
                'total': total,
                'unique_urls': len(set(p.get('url', '') for p in self.collection.find({}, {'url': 1}))),
                'unique_policy_ids': len(set(p.get('policy_id', '') for p in self.collection.find({}, {'policy_id': 1}))),
            },
            'freshness': {
                'total': total,
                'last_7_days': self.collection.count_documents({
                    'crawled_at': {'$gte': (datetime.now() - timedelta(days=7)).isoformat()}
                }),
                'last_30_days': self.collection.count_documents({
                    'crawled_at': {'$gte': (datetime.now() - timedelta(days=30)).isoformat()}
                }),
            },
        }

        # è®¡ç®—å®Œæ•´æ€§ç™¾åˆ†æ¯”
        checks['completeness_percentage'] = {
            field: f"{count / total * 100:.1f}%" if total > 0 else "N/A"
            for field, count in checks['completeness'].items()
        }

        return checks


def print_dashboard(stats: dict, quality: dict):
    """æ‰“å°ç›‘æ§é¢æ¿"""
    print()
    print("â•”" + "â•" * 76 + "â•—")
    print("â•‘" + " " * 76 + "â•‘")
    print("â•‘" + "        å…±äº«CFO - çˆ¬è™«ç›‘æ§é¢æ¿".center(70) + "        â•‘")
    print("â•‘" + " " * 76 + "â•‘")
    print("â• " + "â•" * 76 + "â•£")
    print("â•‘" + " " * 76 + "â•‘")
    print("â•‘" + f"  ç›‘æ§æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}".ljust(76) + "â•‘")
    print("â•‘" + " " * 76 + "â•‘")

    # çˆ¬å–ç»Ÿè®¡
    print("â• " + "â”€" * 76 + "â•£")
    print("â•‘  ğŸ“Š çˆ¬å–ç»Ÿè®¡ (æœ€è¿‘ {} å°æ—¶)".format(stats['period_hours']).ljust(76) + "â•‘")
    print("â•‘" + " " * 76 + "â•‘")
    print(f"â•‘  æ€»æ”¿ç­–æ•°: {stats['total_db']}".ljust(30) + f"æœ€è¿‘çˆ¬å–: {stats['total_recent']}".ljust(46) + "â•‘")
    print("â•‘" + " " * 76 + "â•‘")

    # æŒ‰æ¥æºç»Ÿè®¡
    print("â•‘  ğŸ“Œ æŒ‰æ•°æ®æ¥æº:".ljust(76) + "â•‘")
    for source, count in sorted(stats['source_stats'].items(), key=lambda x: -x[1]):
        print(f"â•‘    {source.ljust(20)} {count} æ¡".ljust(52) + "â•‘")

    print("â•‘" + " " * 76 + "â•‘")

    # æŒ‰å±‚çº§ç»Ÿè®¡
    print("â•‘  ğŸ“š æŒ‰å±‚çº§:".ljust(76) + "â•‘")
    for level, count in sorted(stats['level_stats'].items()):
        print(f"â•‘    {level.ljust(20)} {count} æ¡".ljust(52) + "â•‘")

    print("â•‘" + " " * 76 + "â•‘")

    # æ•°æ®è´¨é‡
    print("â•‘" + "  âœ… æ•°æ®è´¨é‡æ£€æŸ¥:".ljust(76) + "â•‘")
    print("â•‘" + " " * 76 + "â•‘")
    print(f"â•‘    ç¼ºå°‘æ ‡é¢˜: {quality['completeness']['title']} / {stats['total_db']}".ljust(76) + "â•‘")
    print(f"â•‘    ç¼ºå°‘URL: {quality['completeness']['url']} / {stats['total_db']}".ljust(76) + "â•‘")
    print(f"â•‘    ç¼ºå°‘å±‚çº§: {quality['completeness']['document_level']} / {stats['total_db']}".ljust(76) + "â•‘")
    print(f"â•‘    å”¯ä¸€URL: {quality['uniqueness']['unique_urls']}".ljust(76) + "â•‘")
    print(f"â•‘    å”¯ä¸€ID: {quality['uniqueness']['unique_policy_ids']}".ljust(76) + "â•‘")

    print("â•‘" + " " * 76 + "â•‘")

    # æœ€è¿‘çˆ¬å–è¶‹åŠ¿
    print("â•‘  ğŸ“ˆ æœ€è¿‘çˆ¬å–è¶‹åŠ¿:".ljust(76) + "â•‘")
    print("â•‘" + " " * 76 + "â•‘")
    for hour, count in sorted(stats['hourly_stats'].items())[-10:]:
        print(f"â•‘    {hour}: {count} æ¡".ljust(70) + "â•‘")

    print("â•š" + "â•" * 76 + "â•")
    print()


def print_issues(quality: dict):
    """æ‰“å°é—®é¢˜æ¸…å•"""
    issues = []

    if quality['completeness']['title'] > 0:
        issues.append(f"âš ï¸  {quality['completeness']['title']} æ¡æ”¿ç­–ç¼ºå°‘æ ‡é¢˜")

    if quality['completeness']['url'] > 0:
        issues.append(f"âš ï¸  {quality['completeness']['url']} æ¡æ”¿ç­–ç¼ºå°‘URL")

    if quality['uniqueness']['unique_urls'] < quality['uniqueness']['total']:
        duplicate = quality['uniqueness']['total'] - quality['uniqueness']['unique_urls']
        issues.append(f"âš ï¸  å‘ç° {duplicate} æ¡é‡å¤çš„URL")

    if quality['freshness']['last_7_days'] == 0:
        issues.append("âš ï¸  æœ€è¿‘7å¤©æ²¡æœ‰æ–°æ•°æ®çˆ¬å–")

    if quality['freshness']['last_30_days'] == 0:
        issues.append("âš ï¸  æœ€è¿‘30å¤©æ²¡æœ‰æ–°æ•°æ®çˆ¬å–")

    if issues:
        print("\nğŸš¨ å‘ç°çš„é—®é¢˜:")
        print("-" * 50)
        for issue in issues:
            print(issue)
        print()
    else:
        print("\nâœ… æ²¡æœ‰å‘ç°æ˜æ˜¾é—®é¢˜")


def cmd_monitor(args):
    """ç›‘æ§å‘½ä»¤"""
    monitor = CrawlerMonitor(MONGO_CONFIG)
    if not monitor.connect():
        return 1

    try:
        # è·å–ç»Ÿè®¡
        stats = monitor.get_crawl_stats(hours=args.hours)
        quality = monitor.check_data_quality()

        # æ˜¾ç¤ºé¢æ¿
        print_dashboard(stats, quality)

        # æ˜¾ç¤ºé—®é¢˜
        print_issues(quality)

        # å»ºè®®
        print("ğŸ’¡ å»ºè®®:")
        print("-" * 50)
        print("1. å®šæœŸè¿è¡Œæ­¤ç›‘æ§å·¥å…·æ£€æŸ¥çˆ¬è™«çŠ¶æ€")
        print("2. å¦‚å‘ç°é‡å¤æ•°æ®ï¼Œè¿è¡Œå»é‡: python policy_query.py search '' | sort | uniq -d")
        print("3. å¦‚å‘ç°æ•°æ®è´¨é‡ä¸‹é™ï¼Œæ£€æŸ¥çˆ¬è™«æ—¥å¿—")
        print("4. å…³æ³¨çˆ¬å–è¶‹åŠ¿ï¼ŒåŠæ—¶è°ƒæ•´çˆ¬å–ç­–ç•¥")

    finally:
        monitor.disconnect()

    return 0


def cmd_watch(args):
    """å®æ—¶ç›‘æ§æ¨¡å¼"""
    monitor = CrawlerMonitor(MONGO_CONFIG)
    if not monitor.connect():
        return 1

    try:
        print(f"ğŸ”„ å®æ—¶ç›‘æ§æ¨¡å¼ (æ¯ {args.interval} ç§’åˆ·æ–°ï¼ŒæŒ‰ Ctrl+C é€€å‡º)")
        print()

        iteration = 0
        while True:
            iteration += 1
            print(f"\n[{iteration}] åˆ·æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            stats = monitor.get_crawl_stats(hours=1)
            quality = monitor.check_data_quality()

            # ç®€åŒ–æ˜¾ç¤º
            print(f"  æ€»æ•°: {stats['total_db']} | æœ€è¿‘1å°æ—¶: {stats['total_recent']} æ¡")
            print(f"  æ¥æº: {', '.join(f'{k}:{v}' for k, v in stats['source_stats'].items())}")
            print(f"  å±‚çº§: {', '.join(f'{k}:{v}' for k, v in stats['level_stats'].items())}")

            # æ£€æŸ¥é—®é¢˜
            issues = []
            if quality['completeness']['title'] > 10:
                issues.append("ç¼ºå°‘æ ‡é¢˜")
            if quality['uniqueness']['unique_urls'] < quality['uniqueness']['total'] * 0.95:
                issues.append("æœ‰é‡å¤URL")

            if issues:
                print(f"  âš ï¸  é—®é¢˜: {', '.join(issues)}")

            time.sleep(args.interval)

    except KeyboardInterrupt:
        print("\n\nç›‘æ§å·²åœæ­¢")
    finally:
        monitor.disconnect()

    return 0


def cmd_crawler_status(args):
    """æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€...")
    print()

    # æ£€æŸ¥ MongoDB
    try:
        uri = f"mongodb://{MONGO_CONFIG['host']}:{MONGO_CONFIG['port']}"
        client = MongoClient(uri, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')
        print("âœ… MongoDB è¿æ¥æ­£å¸¸")
        client.close()
    except:
        print("âŒ MongoDB è¿æ¥å¤±è´¥")
        return 1

    # æ£€æŸ¥ systemd æœåŠ¡
    try:
        # æ£€æŸ¥æœ¬åœ°çˆ¬è™«æœåŠ¡çŠ¶æ€
        import subprocess
        try:
            result = subprocess.run(['systemctl', 'status', 'shared-cfo-crawler'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print("\nâœ… çˆ¬è™«æœåŠ¡çŠ¶æ€:")
                for line in result.stdout.split('\n')[:10]:
                    print(f"   {line}")
            else:
                print("\nâš ï¸  çˆ¬è™«æœåŠ¡æœªè¿è¡Œæˆ–æœªå®‰è£…")
                print("   å®‰è£…å‘½ä»¤: systemctl enable /opt/shared_cfo/scrapy_crawler.service")
        except Exception as e:
            print(f"\nâš ï¸  æ— æ³•æ£€æŸ¥æœåŠ¡çŠ¶æ€: {e}")
        print(f"\nğŸ“‹ æŸ¥çœ‹çˆ¬è™«æ—¥å¿—:")
        print(f"   tail -50 /opt/shared_cfo/logs/crawler.log")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥æœåŠ¡çŠ¶æ€: {e}")

    return 0


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å…±äº«CFO - çˆ¬è™«ç›‘æ§å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument('--host', default=MONGO_CONFIG['host'], help='MongoDB ä¸»æœº')
    parser.add_argument('--port', type=int, default=MONGO_CONFIG['port'], help='MongoDB ç«¯å£')

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # monitor å‘½ä»¤
    monitor_parser = subparsers.add_parser('monitor', help='ç›‘æ§çˆ¬è™«çŠ¶æ€')
    monitor_parser.add_argument('--hours', type=int, default=24, help='ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰')

    # watch å‘½ä»¤
    watch_parser = subparsers.add_parser('watch', help='å®æ—¶ç›‘æ§')
    watch_parser.add_argument('--interval', type=int, default=30, help='åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰')

    # status å‘½ä»¤
    subparsers.add_parser('status', help='æ£€æŸ¥çˆ¬è™«æœåŠ¡çŠ¶æ€')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    # æ›´æ–°é…ç½®
    MONGO_CONFIG['host'] = args.host
    MONGO_CONFIG['port'] = args.port

    commands = {
        'monitor': cmd_monitor,
        'watch': cmd_watch,
        'status': cmd_crawler_status,
    }

    cmd_func = commands.get(args.command)
    if cmd_func:
        return cmd_func(args)

    return 0


if __name__ == '__main__':
    sys.exit(main())
