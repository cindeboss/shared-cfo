# 共享CFO爬虫 - 手动部署步骤

## 方法1：通过阿里云控制台远程连接（最简单）

### 步骤1：连接到服务器

1. 打开浏览器，访问：https://ecs.console.aliyun.com/
2. 找到您的ECS实例（120.78.5.4）
3. 点击实例名称进入详情页
4. 点击"远程连接"选项卡
5. 选择"VNC连接"或"Workbench连接"
6. 点击"立即连接"

### 步骤2：连接成功后执行命令

在远程连接的终端中，复制粘贴以下全部命令：

```bash
cd /opt/shared-cfo && source venv/bin/activate && cat > crawler.py << 'EOFPY'
#!/usr/bin/env python3
import requests, time, random, logging
from datetime import datetime
from pymongo import MongoClient
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

class TaxCrawler:
    def __init__(self):
        self.base_url = 'https://fgk.chinatax.gov.cn'
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

        password = quote_plus('840307@whY')
        mongo_uri = f'mongodb://root:{password}@dds-wz9acd31e6591e342.mongodb.rds.aliyuncs.com:3717,dds-wz9acd31e6591e341.mongodb.rds.aliyuncs.com:3717/admin?replicaSet=mgset-97608956'

        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s',
            handlers=[logging.FileHandler('/opt/shared-cfo/logs/crawler.log'), logging.StreamHandler()])
        self.logger = logging.getLogger(__name__)
        self.logger.info('Connecting to MongoDB...')
        self.client = MongoClient(mongo_uri, serverSelectionTimeoutMS=10000)
        self.db = self.client['shared_cfo']
        self.collection = self.db['policies']
        self.client.admin.command('ping')
        self.logger.info('MongoDB连接成功')

    def delay(self):
        time.sleep(random.uniform(2.0, 4.0))

    def crawl(self, limit=5):
        self.logger.info(f'开始爬取，目标: {limit}条')
        self.logger.info(f'访问主页查找政策链接...')
        self.delay()

        try:
            response = self.session.get(self.base_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            policies = []

            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                title = link.get_text(strip=True)

                if not href or not title:
                    continue

                if any(kw in title for kw in ['政策', '公告', '通知', '税']):
                    if not href.startswith('http'):
                        from urllib.parse import urljoin
                        full_url = urljoin(self.base_url, href)
                    else:
                        full_url = href

                    policies.append({'title': title, 'url': full_url})

            self.logger.info(f'找到 {len(policies)} 条政策链接')

            success = 0
            for idx, policy in enumerate(policies[:limit], 1):
                title = policy['title']
                url = policy['url']
                self.logger.info(f'[{idx}/{limit}] {title[:50]}')

                if self.crawl_detail(url, title) == 'success':
                    success += 1

            self.logger.info(f'完成 - 成功: {success}')
            return success

        except Exception as e:
            self.logger.error(f'爬取失败: {e}', exc_info=True)
            return 0

    def crawl_detail(self, url, title=None):
        try:
            self.delay()
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            if not title:
                title_elem = soup.find('h1') or soup.find('title')
                title = title_elem.get_text(strip=True) if title_elem else url

            content_div = soup.find('div', class_='content') or soup.find('div', class_='article-content') or soup.find('body')
            content = content_div.get_text(separator='\n', strip=True) if content_div else ''

            policy_id = url.split('/')[-1].replace('.shtml', '').replace('.htm', '')

            doc = {
                'policy_id': policy_id,
                'title': title,
                'source': '国家税务总局',
                'url': url,
                'content': content[:50000],
                'crawled_at': datetime.now(),
                'region': '全国'
            }
            self.collection.insert_one(doc)
            self.logger.info(f'✓ 保存成功')
            return 'success'
        except Exception as e:
            if 'duplicate' in str(e).lower():
                return 'duplicate'
            self.logger.error(f'✗ 失败: {e}')
            return 'error'

if __name__ == '__main__':
    logger = logging.getLogger(__name__)
    logger.info('=' * 40)
    logger.info('共享CFO税务政策爬虫')
    logger.info('=' * 40)
    crawler = TaxCrawler()
    count = crawler.crawl(limit=3)
    logger.info(f'数据库总数: {crawler.collection.count_documents({})}')
    logger.info('完成!')
EOFPY

chmod +x crawler.py && python crawler.py
```

### 步骤3：等待运行完成

运行大约需要 2-5 分钟，会显示进度。

---

## 方法2：手动SSH连接

**在您的本地 PowerShell/CMD 中执行**：

```bash
ssh root@120.78.5.4
# 输入密码：840307@whY
```

连接成功后，执行上面"步骤2"中的命令。

---

## 成功标志

成功的话您会看到：
```
2026-01-17 22:XX:XX - Connecting to MongoDB...
2026-01-17 22:XX:XX - MongoDB连接成功
2026-01-17 22:XX:XX - 开始爬取，目标: 5条
2026-01-17 22:XX:XX - 访问主页查找政策链接...
2026-01-17 22:XX:XX - 找到 X 条政策链接
2026-01-17 22:XX:XX - [1/5] ...
...
2026-01-17 22:XX:XX - ✓ 保存成功
2026-01-17 22:XX:XX - 完成 - 成功: X
2026-01-17 22:XX:XX - 数据库总数: X 条
```

---

**完成后请告诉我执行结果！**

特别是：
1. 是否看到 "MongoDB连接成功"？
2. 是否找到政策链接？
3. 是否有数据保存成功？

如果遇到任何错误，请把完整错误信息发给我。
