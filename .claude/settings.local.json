{
  "permissions": {
    "allow": [
      "Skill(code-review:code-review)",
      "Bash(find:*)",
      "Bash(ls:*)",
      "Bash(wc:*)",
      "Bash(grep:*)",
      "Bash(python3:*)",
      "Bash(ssh:*)",
      "Bash(rsync:*)",
      "Bash(curl:*)",
      "Bash(scp:*)",
      "Bash(chmod:*)",
      "Bash(AK_ID=\"LTAI5t6TidHp1ARfBNRjLG5c\" bash -c 'read -s -p \"\"Access Key Secret: \"\" AK_SECRET && ssh -i ~/.ssh/id_ed25519 root@120.78.5.4 \"\"export ALIYUN_ACCESS_KEY_ID=\\\\\"\"${AK_ID}\\\\\"\" export ALIYUN_ACCESS_KEY_SECRET=\\\\\"\"${AK_SECRET}\\\\\"\" python3 /root/aliyun_open_port_v2.py\"\"')",
      "Bash(for:*)",
      "Bash(do echo \"检查 #$i:\")",
      "Bash(done)",
      "Bash(改进的爬虫服务 - 支持实时进度更新和详细日志)",
      "Bash(\"\"\"\nimport asyncio\nimport logging\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional, Callable\n\n# 添加项目路径\nproject_root = Path\\(__file__\\).parent.parent.parent.parent\nsys.path.insert\\(0, str\\(project_root\\)\\)\n\nfrom crawler.database import MongoDBConnector\nfrom crawler.chinatax_crawler import ChinaTaxCrawler\nfrom crawler.crawler_12366 import Crawler12366\n\nlogger = logging.getLogger\\(__name__\\)\n\n\nclass CrawlProgressTracker:\n    \"\"\"\"\"\"爬虫进度跟踪器\"\"\"\"\"\"\n    \n    def __init__\\(self\\):\n        self.crawled_policies = []\n        self.errors = []\n        self.current_stage = \"\"\"\"\n        self.progress_callback: Optional[Callable] = None\n        self.log_callback: Optional[Callable] = None\n        \n    def set_progress_callback\\(self, callback: Callable[[int, str], None]\\):\n        \"\"\"\"\"\"设置进度回调函数\"\"\"\"\"\"\n        self.progress_callback = callback\n        \n    def set_log_callback\\(self, callback: Callable[[str, str], None]\\):\n        \"\"\"\"\"\"设置日志回调函数 \\(message, level\\)\"\"\"\"\"\"\n        self.log_callback = log_callback\n        \n    def update_progress\\(self, percent: int, message: str\\):\n        \"\"\"\"\"\"更新进度\"\"\"\"\"\"\n        if self.progress_callback:\n            self.progress_callback\\(percent, message\\)\n        self.add_log\\(message, \"\"info\"\"\\)\n        \n    def add_log\\(self, message: str, level: str = \"\"info\"\"\\):\n        \"\"\"\"\"\"添加日志\"\"\"\"\"\"\n        if self.log_callback:\n            self.log_callback\\(message, level\\)\n        logger.info\\(f\"\"[{self.current_stage}] {message}\"\"\\)\n        \n    def log_crawled_policy\\(self, policy_data: Dict[str, Any]\\):\n        \"\"\"\"\"\"记录爬取的政策\"\"\"\"\"\"\n        self.crawled_policies.append\\(policy_data\\)\n        title = policy_data.get\\(''title'', ''Unknown''\\)\n        url = policy_data.get\\(''url'', ''''\\)\n        self.add_log\\(f\"\"爬取成功: {title}\"\", \"\"success\"\"\\)\n        \n    def log_error\\(self, error_msg: str, policy_url: str = \"\"\"\"\\):\n        \"\"\"\"\"\"记录错误\"\"\"\"\"\"\n        error_data = {\n            \"\"time\"\": datetime.now\\(\\).isoformat\\(\\),\n            \"\"message\"\": error_msg,\n            \"\"url\"\": policy_url,\n            \"\"stage\"\": self.current_stage\n        }\n        self.errors.append\\(error_data\\)\n        self.add_log\\(f\"\"错误: {error_msg}\"\", \"\"error\"\"\\)\n        \n    def get_summary\\(self\\) -> Dict[str, Any]:\n        \"\"\"\"\"\"获取爬取摘要\"\"\"\"\"\"\n        return {\n            \"\"total_crawled\"\": len\\(self.crawled_policies\\),\n            \"\"errors\"\": len\\(self.errors\\),\n            \"\"crawled_policies\"\": self.crawled_policies,\n            \"\"errors_detail\"\": self.errors\n        }\n\n\nasync def run_real_crawl_with_progress\\(\n    phase: str = \"\"test\"\",\n    progress_tracker: CrawlProgressTracker = None\n\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"\n    运行真实的爬虫任务，支持实时进度更新\n    \"\"\"\"\"\"\n    result = {\n        \"\"test\"\": \"\"real_crawl\"\",\n        \"\"phase\"\": phase,\n        \"\"start_time\"\": datetime.now\\(\\).isoformat\\(\\),\n        \"\"end_time\"\": None,\n        \"\"total_success\"\": 0,\n        \"\"total_failed\"\": 0,\n        \"\"crawled_details\"\": [],\n        \"\"errors\"\": []\n    }\n    \n    if progress_tracker is None:\n        progress_tracker = CrawlProgressTracker\\(\\)\n    \n    try:\n        # 创建数据库连接\n        db = MongoDBConnector\\(\\)\n        await db.connect\\(\\)\n        \n        if phase == \"\"test\"\":\n            result = await _run_test_crawl\\(db, progress_tracker\\)\n        elif phase == \"\"week1\"\":\n            result = await _run_week1_crawl\\(db, progress_tracker\\)\n        elif phase == \"\"week2\"\":\n            result = await _run_week2_crawl\\(db, progress_tracker\\)\n        elif phase == \"\"week3\"\":\n            result = await _run_week3_crawl\\(db, progress_tracker\\)\n        elif phase == \"\"complete\"\":\n            result = await _run_complete_crawl\\(db, progress_tracker\\)\n        else:\n            raise ValueError\\(f\"\"未知的爬取阶段: {phase}\"\"\\)\n        \n        result[\"\"end_time\"\"] = datetime.now\\(\\).isoformat\\(\\)\n        result[\"\"crawled_details\"\"] = progress_tracker.crawled_policies\n        result[\"\"errors\"\"] = progress_tracker.errors\n        \n        progress_tracker.add_log\\(f\"\"爬虫任务完成！成功: {result.get\\(''total_success'', 0\\)}, 失败: {result.get\\(''total_failed'', 0\\)}\"\", \"\"success\"\"\\)\n        \n        logger.info\\(f\"\"爬虫任务完成: {result}\"\"\\)\n        return result\n        \n    except Exception as e:\n        logger.error\\(f\"\"真实爬虫执行失败: {e}\"\"\\)\n        progress_tracker.log_error\\(f\"\"爬虫执行失败: {str\\(e\\)}\"\"\\)\n        result[\"\"total_failed\"\"] = -1\n        result[\"\"errors\"\"].append\\({\n            \"\"time\"\": datetime.now\\(\\).isoformat\\(\\),\n            \"\"message\"\": f\"\"爬虫执行失败: {str\\(e\\)}\"\"\n        }\\)\n        raise\n    finally:\n        await db.disconnect\\(\\)\n\n\nasync def _run_test_crawl\\(db: MongoDBConnector, tracker: CrawlProgressTracker\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"测试阶段爬取 - 爬取少量数据\"\"\"\"\"\"\n    tracker.current_stage = \"\"测试爬取\"\"\n    tracker.update_progress\\(5, \"\"开始测试爬取...\"\"\\)\n    \n    result = {\n        \"\"total_success\"\": 0,\n        \"\"total_failed\"\": 0\n    }\n    \n    # 爬取法律\n    crawler = ChinaTaxCrawler\\(db\\)\n    \n    try:\n        tracker.update_progress\\(10, \"\"正在爬取国家税务总局法律...\"\"\\)\n        \n        law_stats = await asyncio.to_thread\\(crawler.crawl_laws, 1\\)\n        \n        result[\"\"total_success\"\"] = law_stats.get\\(\"\"success\"\", 0\\)\n        result[\"\"total_failed\"\"] = law_stats.get\\(\"\"failed\"\", 0\\)\n        \n        tracker.update_progress\\(50, f\"\"法律爬取完成: 成功 {law_stats.get\\(''success'', 0\\)}, 失败 {law_stats.get\\(''failed'', 0\\)}\"\"\\)\n        \n        # 爬取12366问答\n        tracker.update_progress\\(60, \"\"正在爬取12366问答...\"\"\\)\n        qa_crawler = Crawler12366\\(db\\)\n        \n        qa_stats = await asyncio.to_thread\\(qa_crawler.crawl_hot_qa, 2\\)\n        \n        result[\"\"total_success\"\"] += qa_stats.get\\(\"\"success\"\", 0\\)\n        result[\"\"total_failed\"\"] += qa_stats.get\\(\"\"failed\"\", 0\\)\n        \n        tracker.update_progress\\(90, f\"\"问答爬取完成: 成功 {qa_stats.get\\(''success'', 0\\)}, 失败 {qa_stats.get\\(''failed'', 0\\)}\"\"\\)\n        \n        qa_crawler.close\\(\\)\n        \n    finally:\n        crawler.close\\(\\)\n    \n    return result\n\n\nasync def _run_week1_crawl\\(db: MongoDBConnector, tracker: CrawlProgressTracker\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"第一周阶段爬取\"\"\"\"\"\"\n    tracker.current_stage = \"\"第一周爬取\"\"\n    tracker.update_progress\\(5, \"\"开始第一周爬取...\"\"\\)\n    \n    result = {\"\"total_success\"\": 0, \"\"total_failed\"\": 0}\n    \n    crawler = ChinaTaxCrawler\\(db\\)\n    \n    try:\n        tracker.update_progress\\(10, \"\"开始爬取法律\"\"\\)\n        \n        # 爬取法律 \\(5条\\)\n        law_stats = await asyncio.to_thread\\(crawler.crawl_laws, 5\\)\n        result[\"\"total_success\"\"] += law_stats.get\\(\"\"success\"\", 0\\)\n        result[\"\"total_failed\"\"] += law_stats.get\\(\"\"failed\"\", 0\\)\n        \n        tracker.update_progress\\(30, f\"\"法律完成: {law_stats.get\\(''success'', 0\\)}条\"\"\\)\n        \n        # 爬取行政法规 \\(25条\\)\n        tracker.update_progress\\(35, \"\"开始爬取行政法规\"\"\\)\n        \n        reg_stats = await asyncio.to_thread\\(crawler.crawl_regulations, 25\\)\n        result[\"\"total_success\"\"] += reg_stats.get\\(\"\"success\"\", 0\\)\n        result[\"\"total_failed\"\"] += reg_stats.get\\(\"\"failed\"\", 0\\)\n        \n        tracker.update_progress\\(65, f\"\"行政法规完成: {reg_stats.get\\(''success'', 0\\)}条\"\"\\)\n        \n        # 爬取部门规章 \\(500条\\)\n        tracker.update_progress\\(70, \"\"开始爬取部门规章\"\"\\)\n        \n        rules_stats = await asyncio.to_thread\\(crawler.crawl_rules, 500\\)\n        result[\"\"total_success\"\"] += rules_stats.get\\(\"\"success\"\", 0\\)\n        result[\"\"total_failed\"\"] += rules_stats.get\\(\"\"failed\"\", 0\\)\n        \n        tracker.update_progress\\(95, f\"\"部门规章完成: {rules_stats.get\\(''success'', 0\\)}条\"\"\\)\n        \n    finally:\n        crawler.close\\(\\)\n    \n    return result\n\n\nasync def _run_week2_crawl\\(db: MongoDBConnector, tracker: CrawlProgressTracker\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"第二周阶段爬取\"\"\"\"\"\"\n    tracker.current_stage = \"\"第二周爬取\"\"\n    tracker.update_progress\\(5, \"\"开始第二周爬取...\"\"\\)\n    \n    result = {\"\"total_success\"\": 0, \"\"total_failed\"\": 0}\n    tracker.update_progress\\(100, \"\"第二周爬取完成\"\"\\)\n    \n    return result\n\n\nasync def _run_week3_crawl\\(db: MongoDBConnector, tracker: CrawlProgressTracker\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"第三周阶段爬取\"\"\"\"\"\"\n    tracker.current_stage = \"\"第三周爬取\"\"\n    tracker.update_progress\\(5, \"\"开始第三周爬取...\"\"\\)\n    \n    result = {\"\"total_success\"\": 0, \"\"total_failed\"\": 0}\n    tracker.update_progress\\(100, \"\"第三周爬取完成\"\"\\)\n    \n    return result\n\n\nasync def _run_complete_crawl\\(db: MongoDBConnector, tracker: CrawlProgressTracker\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"完整爬取\"\"\"\"\"\"\n    tracker.current_stage = \"\"完整爬取\"\"\n    tracker.update_progress\\(1, \"\"开始完整爬取...\"\"\\)\n    \n    # 运行所有阶段\n    result1 = await _run_week1_crawl\\(db, tracker\\)\n    result2 = await _run_week2_crawl\\(db, tracker\\)\n    result3 = await _run_week3_crawl\\(db, tracker\\)\n    \n    return {\n        \"\"total_success\"\": result1[\"\"total_success\"\"] + result2[\"\"total_success\"\"] + result3[\"\"total_success\"\"],\n        \"\"total_failed\"\": result1[\"\"total_failed\"\"] + result2[\"\"total_failed\"\"] + result3[\"\"total_failed\"\"]\n    }\n\n\n# 导出主函数\nasync def run_real_crawl\\(phase: str = \"\"test\"\"\\) -> Dict[str, Any]:\n    \"\"\"\"\"\"运行真实爬虫的入口函数（无回调）\"\"\"\"\"\"\n    tracker = CrawlProgressTracker\\(\\)\n    return await run_real_crawl_with_progress\\(phase, tracker\\)\nEOFPYTHON\n\")",
      "Bash(do echo \"=== Check $i ===\")",
      "Bash(git add:*)",
      "Bash(git commit:*)"
    ]
  }
}
